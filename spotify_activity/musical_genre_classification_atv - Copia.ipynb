{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Imports](#toc1_)    \n",
    "- [Read Data](#toc2_)    \n",
    "- [Data Preprocessing](#toc3_)\n",
    "  - [Drop rows with missing values](#toc3_2_)    \n",
    "  - [Removing Categorical Columns](#toc4_)    \n",
    "  - [Split Train and Test Data](#toc5_)    \n",
    "  - [Data Cleaning](#toc6_)    \n",
    "    - [Impute missing numeric data](#toc6_1_)    \n",
    "  - [Data Normalization](#toc7_)    \n",
    "- [Model training](#toc8_)    \n",
    "  - [KNN](#toc8_1_)  \n",
    "  - [LVQ](#toc8_2_)\n",
    "  - [Decision Tree](#toc8_3_)  \n",
    "  - [MLP](#toc8_4_)\n",
    "  - [SVM](#toc8_5_)  \n",
    "  - [Stacking](#toc8_6_)  \n",
    "  - [Random Forest](#toc8_7_)  \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_validate,train_test_split, GridSearchCV\n",
    "from sklvq import GLVQ\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, make_scorer\n",
    "import random\n",
    "from random import seed,randrange\n",
    "import requests\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/rickvanveen/sklvq.git\n",
      "  Cloning https://github.com/rickvanveen/sklvq.git to c:\\users\\pichau\\appdata\\local\\temp\\pip-req-build-8fam6gx1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/rickvanveen/sklvq.git 'C:\\Users\\pichau\\AppData\\Local\\Temp\\pip-req-build-8fam6gx1'\n",
      "  fatal: unable to access 'https://github.com/rickvanveen/sklvq.git/': Could not resolve host: github.com\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × git clone --filter=blob:none --quiet https://github.com/rickvanveen/sklvq.git 'C:\\Users\\pichau\\AppData\\Local\\Temp\\pip-req-build-8fam6gx1' did not run successfully.\n",
      "  │ exit code: 128\n",
      "  ╰─> See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× git clone --filter=blob:none --quiet https://github.com/rickvanveen/sklvq.git 'C:\\Users\\pichau\\AppData\\Local\\Temp\\pip-req-build-8fam6gx1' did not run successfully.\n",
      "│ exit code: 128\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install -U git+https://github.com/rickvanveen/sklvq.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Read Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the csv file from your GitHub account\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Zuluke/Projetos-AM/main/spotify_activity/dataset.csv\" # Make sure the url is the raw version of the file on GitHub\n",
    "download = requests.get(url).content\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Visualize Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114000, 21)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114000 entries, 0 to 113999\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Unnamed: 0        114000 non-null  int64  \n",
      " 1   track_id          114000 non-null  object \n",
      " 2   artists           113999 non-null  object \n",
      " 3   album_name        113999 non-null  object \n",
      " 4   track_name        113999 non-null  object \n",
      " 5   popularity        114000 non-null  int64  \n",
      " 6   duration_ms       114000 non-null  int64  \n",
      " 7   explicit          114000 non-null  bool   \n",
      " 8   danceability      114000 non-null  float64\n",
      " 9   energy            114000 non-null  float64\n",
      " 10  key               114000 non-null  int64  \n",
      " 11  loudness          114000 non-null  float64\n",
      " 12  mode              114000 non-null  int64  \n",
      " 13  speechiness       114000 non-null  float64\n",
      " 14  acousticness      114000 non-null  float64\n",
      " 15  instrumentalness  114000 non-null  float64\n",
      " 16  liveness          114000 non-null  float64\n",
      " 17  valence           114000 non-null  float64\n",
      " 18  tempo             114000 non-null  float64\n",
      " 19  time_signature    114000 non-null  int64  \n",
      " 20  track_genre       114000 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(6), object(5)\n",
      "memory usage: 17.5+ MB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Soundtrack)</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                track_id                 artists  \\\n",
       "0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
       "1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "\n",
       "                                               album_name  \\\n",
       "0                                                  Comedy   \n",
       "1                                        Ghost (Acoustic)   \n",
       "2                                          To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Soundtrack)   \n",
       "4                                                 Hold On   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "0                      Comedy          73       230666     False   \n",
       "1            Ghost - Acoustic          55       149610     False   \n",
       "2              To Begin Again          57       210826     False   \n",
       "3  Can't Help Falling In Love          71       201933     False   \n",
       "4                     Hold On          82       198853     False   \n",
       "\n",
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
       "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
       "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
       "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
       "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
       "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
       "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
       "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
       "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
       "4          0.000000    0.0829    0.167  119.949               4    acoustic  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "print(df.shape)\n",
    "print('\\n')\n",
    "df.info()\n",
    "print('\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data Preprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Drop rows with missing values](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True, axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Removing Categorical Columns](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 113999 entries, 0 to 113999\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   popularity        113999 non-null  int64  \n",
      " 1   duration_ms       113999 non-null  int64  \n",
      " 2   explicit          113999 non-null  bool   \n",
      " 3   danceability      113999 non-null  float64\n",
      " 4   energy            113999 non-null  float64\n",
      " 5   key               113999 non-null  int64  \n",
      " 6   loudness          113999 non-null  float64\n",
      " 7   mode              113999 non-null  int64  \n",
      " 8   speechiness       113999 non-null  float64\n",
      " 9   acousticness      113999 non-null  float64\n",
      " 10  instrumentalness  113999 non-null  float64\n",
      " 11  liveness          113999 non-null  float64\n",
      " 12  valence           113999 non-null  float64\n",
      " 13  tempo             113999 non-null  float64\n",
      " 14  time_signature    113999 non-null  int64  \n",
      " 15  track_genre       113999 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(5), object(1)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = [\"Unnamed: 0\", \"track_id\", \"track_name\", \"album_name\", \"artists\"]\n",
    "df = df.drop(categorical_columns, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Split Train and Test Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(df, target_column, validation_size=0.1, test_size=0.1, random_state=42):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_column])\n",
    "    \n",
    "    df_train, df_validation = train_test_split(df_train,\n",
    "                                               test_size=validation_size/(1 - test_size),\n",
    "                                               random_state=random_state,\n",
    "                                               stratify=df_train[target_column])\n",
    "    return df_train, df_validation, df_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 113999 entries, 0 to 113999\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   popularity        113999 non-null  int64  \n",
      " 1   duration_ms       113999 non-null  int64  \n",
      " 2   explicit          113999 non-null  bool   \n",
      " 3   danceability      113999 non-null  float64\n",
      " 4   energy            113999 non-null  float64\n",
      " 5   key               113999 non-null  int64  \n",
      " 6   loudness          113999 non-null  float64\n",
      " 7   mode              113999 non-null  int64  \n",
      " 8   speechiness       113999 non-null  float64\n",
      " 9   acousticness      113999 non-null  float64\n",
      " 10  instrumentalness  113999 non-null  float64\n",
      " 11  liveness          113999 non-null  float64\n",
      " 12  valence           113999 non-null  float64\n",
      " 13  tempo             113999 non-null  float64\n",
      " 14  time_signature    113999 non-null  int64  \n",
      " 15  track_genre       113999 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(5), object(1)\n",
      "memory usage: 14.0+ MB\n",
      "\n",
      " 0.5999964911972913 0.2000017544013544 0.2000017544013544\n"
     ]
    }
   ],
   "source": [
    "df_train, df_validation, df_test = train_validation_test_split(df, \"track_genre\",0.2, 0.2)\n",
    "df.info()\n",
    "\n",
    "print('\\n',len(df_train.values)/float(len(df)),len(df_test.values)/float(len(df)),len(df_validation.values)/float(len(df))) #Garantindo que o percentual ocorre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Data Cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_'></a>[Impute missing numeric data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "numeric_imputer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = numeric_imputer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = numeric_imputer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = numeric_imputer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_'></a>[Data Normalization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "\n",
    "normalizer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = normalizer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = normalizer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = normalizer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Div. de dados atributos e classe\n",
    "df_cara_train = df_train[numeric_columns].values  #caracteristicas\n",
    "df_clas_train = df_train['track_genre'].values #classe\n",
    "\n",
    "df_cara_validation = df_validation[numeric_columns].values  #caracteristicas\n",
    "df_clas_validation = df_validation['track_genre'].values #classe\n",
    "\n",
    "df_cara_test = df_test[numeric_columns].values  #caracteristicas\n",
    "df_clas_test = df_test['track_genre'].values #classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Model training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[KNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cara_train_scaled = df_cara_train\n",
    "df_cara_valid_scaled = df_cara_validation\n",
    "df_cara_test_scaled = df_cara_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1748636  0.15861048 0.16880577 0.17154974 0.17500064 0.17567899\n",
      " 0.17537008 0.17594873 0.17530662 0.17495474 0.17563676 0.17429404\n",
      " 0.17423267 0.17333656 0.17353709 0.1732115  0.17287979 0.17233861\n",
      " 0.17125046 0.17021634 0.1702484  0.16896469 0.16769784 0.16724919\n",
      " 0.16641829 0.16637438 0.16600326 0.16487446 0.16456805 0.16393318\n",
      " 0.16359721 0.16302103 0.16188695 0.16125885 0.16108099 0.16073382\n",
      " 0.16078705 0.15982873 0.1588715  0.15828664 0.18945929 0.17724955\n",
      " 0.18825781 0.19278721 0.19557491 0.19736664 0.19715771 0.19794149\n",
      " 0.19734906 0.1991353  0.19896836 0.19766001 0.19836991 0.19772185\n",
      " 0.19737806 0.19758553 0.19734314 0.19711047 0.19644256 0.19637877\n",
      " 0.19560007 0.19604323 0.19556347 0.19502116 0.1942498  0.19444474\n",
      " 0.19418936 0.19393728 0.19386993 0.19266551 0.19213404 0.19144797\n",
      " 0.19135651 0.19105361 0.19006619 0.18963809 0.19024574 0.19016178\n",
      " 0.18950826 0.1891294 ] \n",
      "\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 19}\n",
      "Melhor resultado: 0.1991353044549033 \n",
      "\n",
      "\n",
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier().fit(df_cara_train,df_clas_train)\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1,81,2),\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(df_cara_train,df_clas_train)\n",
    "\n",
    "with open('KNN_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search,f)\n",
    "\n",
    "print(grid_search.cv_results_['mean_test_score'],'\\n\\n')\n",
    "print(f'Melhor parametro: {grid_search.best_params_}')\n",
    "print(f'Melhor resultado: {grid_search.best_score_}','\\n\\n')\n",
    "\n",
    "best_knn = grid_search.best_estimator_\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_cara_test)\n",
    "\n",
    "\n",
    "\n",
    "# Avaliando a Acurácia, recall, F1 score, precisão e roc_auc\n",
    "\n",
    "# Evaluating accuracy, recall, F1 score, and precision with the trained data\n",
    "accuracy = best_knn.score(df_cara_train_scaled, df_clas_train)\n",
    "recall = recall_score(df_clas_train, best_knn.predict(df_cara_train_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_train, best_knn.predict(df_cara_train_scaled), average='macro')\n",
    "precision = precision_score(df_clas_train, best_knn.predict(df_cara_train_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_train = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Evaluating accuracy, recall, F1 score, and precision with the test data\n",
    "accuracy = best_knn.score(df_cara_test_scaled, df_clas_test)\n",
    "recall = recall_score(df_clas_test, best_knn.predict(df_cara_test_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_test, best_knn.predict(df_cara_test_scaled), average='macro')\n",
    "precision = precision_score(df_clas_test, best_knn.predict(df_cara_test_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_test = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Avaliando a Acurácia, recall, F1 score, precisão\n",
    "accuracy = best_knn.score(df_cara_valid_scaled, df_clas_validation)\n",
    "recall = recall_score(df_clas_validation, best_knn.predict(df_cara_valid_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_validation, best_knn.predict(df_cara_valid_scaled), average='macro')\n",
    "precision = precision_score(df_clas_validation, best_knn.predict(df_cara_valid_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_validation = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "\n",
    "print(f'Dados de Teste')\n",
    "print(evaluation_train)\n",
    "print(evaluation_validation)\n",
    "print(evaluation_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_2_'></a>[LVQ](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cara_train_scaled = df_cara_train\n",
    "df_cara_valid_scaled = df_cara_validation\n",
    "df_cara_test_scaled = df_cara_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o classificador LVQ\n",
    "lvq = GLVQ()\n",
    "\n",
    "# Criando o dicionário de parâmetros para o grid search\n",
    "param_grid = {\n",
    "    \"prototype_n_per_class\": [1,3],  # Número de protótipos por classe\n",
    "    \"distance_type\": [\"euclidean\"],\n",
    "    \"solver_params\": [{\"max_runs\": 5, \"step_size\": step} for step in [0.1, 0.5]]  # Lista de dicionários para diferentes step_sizes\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "# Criando os scorers personalizados\n",
    "scorers = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision_macro\": make_scorer(precision_score, average='macro', zero_division = 0),\n",
    "    \"recall_macro\": make_scorer(recall_score, average='macro'),\n",
    "    \"f1_macro\": make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(lvq, param_grid, cv=5, scoring=scorers, refit=\"accuracy\")\n",
    "\n",
    "# Treinando o GridSearchCV com os dados de treino escalados\n",
    "grid_search.fit(df_cara_train_scaled, df_clas_train)\n",
    "\n",
    "with open('LVQ_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros:  {'distance_type': 'euclidean', 'prototype_n_per_class': 3, 'solver_params': {'max_runs': 5, 'step_size': 0.1}}\n",
      "accuracy per fold:  [0.12982456 0.12938596 0.13377193 0.13333333 0.13881579]\n",
      "recall_macro per fold:  [0.12982456 0.12938596 0.13377193 0.13333333 0.13881579]\n",
      "f1_macro per fold:  [0.1140344  0.11959892 0.11408463 0.11836989 0.12084056]\n",
      "precision_macro per fold:  [0.12466036 0.13964932 0.14156033 0.1312936  0.13110597]\n",
      "13.302631578947368\n",
      "13.302631578947368\n",
      "11.738567900442213\n",
      "13.365391757392203\n",
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "# Os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros: \", grid_search.best_params_)\n",
    "\n",
    "# O melhor classificador encontrado pelo grid search\n",
    "\n",
    "best_lvq = grid_search.best_estimator_\n",
    "\n",
    "# Avaliando a Acurácia, recall, F1 score, precisão e roc_auc\n",
    "\n",
    "# Evaluating accuracy, recall, F1 score, and precision with the trained data\n",
    "accuracy = best_lvq.score(df_cara_train_scaled, df_clas_train)\n",
    "recall = recall_score(df_clas_train, best_lvq.predict(df_cara_train_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_train, best_lvq.predict(df_cara_train_scaled), average='macro')\n",
    "precision = precision_score(df_clas_train, best_lvq.predict(df_cara_train_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_train = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Evaluating accuracy, recall, F1 score, and precision with the trained data\n",
    "accuracy = best_lvq.score(df_cara_test_scaled, df_clas_test)\n",
    "recall = recall_score(df_clas_test, best_lvq.predict(df_cara_test_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_test, best_lvq.predict(df_cara_test_scaled), average='macro')\n",
    "precision = precision_score(df_clas_test, best_lvq.predict(df_cara_test_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_test = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Avaliando a Acurácia, recall, F1 score, precisão e roc_auc\n",
    "accuracy = best_lvq.score(df_cara_valid_scaled, df_clas_validation)\n",
    "recall = recall_score(df_clas_validation, best_lvq.predict(df_cara_valid_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_validation, best_lvq.predict(df_cara_valid_scaled), average='macro')\n",
    "precision = precision_score(df_clas_validation, best_lvq.predict(df_cara_valid_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_validation = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "LVQ_best = GLVQ(distance_type='euclidean', prototype_n_per_class=3, solver_params={\"max_runs\": 5, \"step_size\": 0.1})\n",
    "\n",
    "LVQ_best.fit(df_cara_train_scaled, df_clas_train)\n",
    "\n",
    "evaluation_avg = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "\n",
    "# 10 folds cross validation\n",
    "\n",
    "cv_results = cross_validate(LVQ_best, df_cara_valid_scaled, df_clas_validation, cv=5, scoring=evaluation_avg)\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "# results per metric\n",
    "for metric in evaluation_avg:\n",
    "    print(f\"{metric} per fold: \", cv_results[f'test_{metric}'])\n",
    "    results_df[f'{metric}_per_fold'] = cv_results[f'test_{metric}']\n",
    "\n",
    "for i in range(4):\n",
    "    print(np.mean(results_df.values[:,i])*100)\n",
    "\n",
    "print(f'Dados de Teste')\n",
    "print(evaluation_train)\n",
    "print(evaluation_validation)\n",
    "print(evaluation_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_3_'></a>[Decision Tree](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_4_'></a>[MLP](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cara_train_scaled = df_cara_train\n",
    "df_cara_valid_scaled = df_cara_validation\n",
    "df_cara_test_scaled = df_cara_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.296 total time= 7.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.291 total time= 7.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.310 total time= 6.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.307 total time= 8.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.302 total time= 7.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.301 total time= 2.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.299 total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.305 total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.303 total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=700, solver=adam;, score=0.305 total time= 2.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (700) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(),\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;], &#x27;alpha&#x27;: [0.0001],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(100,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;adaptive&#x27;], &#x27;max_iter&#x27;: [700],\n",
       "                         &#x27;solver&#x27;: [&#x27;adam&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(),\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;], &#x27;alpha&#x27;: [0.0001],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(100,)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;adaptive&#x27;], &#x27;max_iter&#x27;: [700],\n",
       "                         &#x27;solver&#x27;: [&#x27;adam&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(),\n",
       "             param_grid={'activation': ['relu', 'tanh'], 'alpha': [0.0001],\n",
       "                         'hidden_layer_sizes': [(100,)],\n",
       "                         'learning_rate': ['adaptive'], 'max_iter': [700],\n",
       "                         'solver': ['adam']},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'], #, 'sgd'],\n",
    "    'alpha': [0.0001],# 0.01],\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'max_iter': [700]#300,500,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', verbose=3)\n",
    "grid_search.fit(df_cara_train, df_clas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3010278  0.30273837]\n",
      "Melhor parametro: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 700, 'solver': 'adam'}\n",
      "Melhor resultado: 0.30273837281631594\n"
     ]
    }
   ],
   "source": [
    "with open('MLP_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search, f)\n",
    "print(grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Melhor parametro: {grid_search.best_params_}')\n",
    "print(f'Melhor resultado: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "# Predizendo os rótulos dos dados de teste\n",
    "best_mlp = grid_search.best_estimator_\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_cara_test)\n",
    "\n",
    "accuracy = best_mlp.score(df_cara_train_scaled, df_clas_train)\n",
    "recall = recall_score(df_clas_train, best_mlp.predict(df_cara_train_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_train, best_mlp.predict(df_cara_train_scaled), average='macro')\n",
    "precision = precision_score(df_clas_train, best_mlp.predict(df_cara_train_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_train = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Evaluating accuracy, recall, F1 score, and precision with the trained data\n",
    "accuracy = best_mlp.score(df_cara_test_scaled, df_clas_test)\n",
    "recall = recall_score(df_clas_test, best_mlp.predict(df_cara_test_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_test, best_mlp.predict(df_cara_test_scaled), average='macro')\n",
    "precision = precision_score(df_clas_test, best_mlp.predict(df_cara_test_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_test = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Avaliando a Acurácia, recall, F1 score, precisão e roc_auc\n",
    "accuracy = best_mlp.score(df_cara_valid_scaled, df_clas_validation)\n",
    "recall = recall_score(df_clas_validation, best_mlp.predict(df_cara_valid_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_validation, best_mlp.predict(df_cara_valid_scaled), average='macro')\n",
    "precision = precision_score(df_clas_validation, best_mlp.predict(df_cara_valid_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_validation = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "print(f'Dados de Teste')\n",
    "print(evaluation_train)\n",
    "print(evaluation_validation)\n",
    "print(evaluation_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_5_'></a>[SVM](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_svm = SVC().fit(df_cara_train,df_clas_train)\n",
    "###CUIDADO AO RODAR AS CÉLULAS ABAIXO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_kernels=['rbf']\n",
    "lista_c =[100]\n",
    "lista_gamma = [2]\n",
    "\n",
    "# Criando um dicionário com os hiperparâmetros e valores a serem testados\n",
    "param_grid = {'kernel': lista_kernels,'C': lista_c, 'gamma':lista_gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2449159]\n",
      "Melhor parametro: {'C': 100, 'gamma': 2, 'kernel': 'rbf'}\n",
      "Melhor resultado: 0.24491589853230442\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(class_svm, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(df_cara_train,df_clas_train)\n",
    "\n",
    "with open('SVM_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search,f)\n",
    "print(grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Melhor parametro: {grid_search.best_params_}')\n",
    "print(f'Melhor resultado: {grid_search.best_score_}')\n",
    "#lista_kernels=['linear','rbf']\n",
    "#lista_c =[2,3,4,5,7,10,100]\n",
    "#lista_gamma = [2,3,4,5,7,10,100]\n",
    "#Melhor parametro: {'C': 100, 'gamma': 2, 'kernel': 'rbf'}\n",
    "#Melhor resultado: 0.26281478175137607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "# Predizendo os rótulos dos dados de teste\n",
    "best_mlp = grid_search.best_estimator_\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_cara_test)\n",
    "\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_cara_test)\n",
    "\n",
    "accuracy = best_mlp.score(df_cara_train_scaled, df_clas_train)\n",
    "recall = recall_score(df_clas_train, best_mlp.predict(df_cara_train_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_train, best_mlp.predict(df_cara_train_scaled), average='macro')\n",
    "precision = precision_score(df_clas_train, best_mlp.predict(df_cara_train_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_train = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Evaluating accuracy, recall, F1 score, and precision with the trained data\n",
    "accuracy = best_mlp.score(df_cara_test_scaled, df_clas_test)\n",
    "recall = recall_score(df_clas_test, best_mlp.predict(df_cara_test_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_test, best_mlp.predict(df_cara_test_scaled), average='macro')\n",
    "precision = precision_score(df_clas_test, best_mlp.predict(df_cara_test_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_test = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "# Avaliando a Acurácia, recall, F1 score, precisão e roc_auc\n",
    "accuracy = best_mlp.score(df_cara_valid_scaled, df_clas_validation)\n",
    "recall = recall_score(df_clas_validation, best_mlp.predict(df_cara_valid_scaled), average='macro')\n",
    "f1 = f1_score(df_clas_validation, best_mlp.predict(df_cara_valid_scaled), average='macro')\n",
    "precision = precision_score(df_clas_validation, best_mlp.predict(df_cara_valid_scaled), average='macro', zero_division=0)\n",
    "\n",
    "evaluation_validation = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "    'precision_macro': make_scorer(precision_score, average='macro', zero_division = 0)\n",
    "}\n",
    "\n",
    "print(f'Dados de Teste')\n",
    "print(evaluation_train)\n",
    "print(evaluation_validation)\n",
    "print(evaluation_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_6_'></a>[Stacking](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_7_'></a>[Random Forest](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
