{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Imports](#toc1_)    \n",
    "- [Read Data](#toc2_)    \n",
    "- [Data Selection](#toc3_)    \n",
    "  - [Drop trash columns](#toc3_1_)    \n",
    "  - [Drop rows with missing values](#toc3_2_)    \n",
    "  - [Selecting 50% of the data](#toc3_3_)    \n",
    "- [Removing Categorical Columns](#toc4_)    \n",
    "- [Split Train and Test Data](#toc5_)    \n",
    "- [Data Cleaning](#toc6_)    \n",
    "  - [Impute missing numeric data](#toc6_1_)    \n",
    "- [Data Normalization](#toc7_)    \n",
    "- [Model training](#toc8_)    \n",
    "  - [KNN](#toc8_1_)    \n",
    "    - [Best model](#toc8_1_1_)    \n",
    "  - [LVQ](#toc8_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Read Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (file://C:/Users/pichau/.cache/huggingface/datasets/maharshipandya___csv/maharshipandya--spotify-tracks-dataset-ff79c8444e5ec4c3/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaharshipandya/spotify-tracks-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#df = pd.read_csv(\"C:/Users/bito-/OneDrive - UFPE/Área de Trabalho/Bito/Exatas/AM/AM 2024.1/Atividades Leandro/Emerson/atividade_01_10042024/dados/dataset.csv\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1810\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n\u001b[1;32m-> 1810\u001b[0m ds \u001b[38;5;241m=\u001b[39m builder_instance\u001b[38;5;241m.\u001b[39mas_dataset(split\u001b[38;5;241m=\u001b[39msplit, verification_mode\u001b[38;5;241m=\u001b[39mverification_mode, in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory)\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1107\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[0;32m   1105\u001b[0m is_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a dataset cached in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir):\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1110\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: could not find data in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please make sure to call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilder.download_and_prepare(), or use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"maharshipandya/spotify-tracks-dataset\")\n",
    "#df = pd.read_csv(\"C:/Users/bito-/OneDrive - UFPE/Área de Trabalho/Bito/Exatas/AM/AM 2024.1/Atividades Leandro/Emerson/atividade_01_10042024/dados/dataset.csv\")\n",
    "\n",
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114000, 21)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114000 entries, 0 to 113999\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Unnamed: 0        114000 non-null  int64  \n",
      " 1   track_id          114000 non-null  object \n",
      " 2   artists           113999 non-null  object \n",
      " 3   album_name        113999 non-null  object \n",
      " 4   track_name        113999 non-null  object \n",
      " 5   popularity        114000 non-null  int64  \n",
      " 6   duration_ms       114000 non-null  int64  \n",
      " 7   explicit          114000 non-null  bool   \n",
      " 8   danceability      114000 non-null  float64\n",
      " 9   energy            114000 non-null  float64\n",
      " 10  key               114000 non-null  int64  \n",
      " 11  loudness          114000 non-null  float64\n",
      " 12  mode              114000 non-null  int64  \n",
      " 13  speechiness       114000 non-null  float64\n",
      " 14  acousticness      114000 non-null  float64\n",
      " 15  instrumentalness  114000 non-null  float64\n",
      " 16  liveness          114000 non-null  float64\n",
      " 17  valence           114000 non-null  float64\n",
      " 18  tempo             114000 non-null  float64\n",
      " 19  time_signature    114000 non-null  int64  \n",
      " 20  track_genre       114000 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(6), object(5)\n",
      "memory usage: 17.5+ MB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Soundtrack)</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                track_id                 artists  \\\n",
       "0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
       "1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "\n",
       "                                               album_name  \\\n",
       "0                                                  Comedy   \n",
       "1                                        Ghost (Acoustic)   \n",
       "2                                          To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Soundtrack)   \n",
       "4                                                 Hold On   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "0                      Comedy          73       230666     False   \n",
       "1            Ghost - Acoustic          55       149610     False   \n",
       "2              To Begin Again          57       210826     False   \n",
       "3  Can't Help Falling In Love          71       201933     False   \n",
       "4                     Hold On          82       198853     False   \n",
       "\n",
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
       "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
       "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
       "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
       "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
       "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
       "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
       "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
       "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
       "4          0.000000    0.0829    0.167  119.949               4    acoustic  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "print(df.shape)\n",
    "print('\\n')\n",
    "df.info()\n",
    "print('\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_genre\n",
       "acoustic             0.877193\n",
       "punk-rock            0.877193\n",
       "progressive-house    0.877193\n",
       "power-pop            0.877193\n",
       "pop                  0.877193\n",
       "pop-film             0.877193\n",
       "piano                0.877193\n",
       "party                0.877193\n",
       "pagode               0.877193\n",
       "opera                0.877193\n",
       "new-age              0.877193\n",
       "mpb                  0.877193\n",
       "minimal-techno       0.877193\n",
       "metalcore            0.877193\n",
       "metal                0.877193\n",
       "mandopop             0.877193\n",
       "malay                0.877193\n",
       "latino               0.877193\n",
       "latin                0.877193\n",
       "kids                 0.877193\n",
       "k-pop                0.877193\n",
       "jazz                 0.877193\n",
       "j-rock               0.877193\n",
       "j-pop                0.877193\n",
       "j-idol               0.877193\n",
       "j-dance              0.877193\n",
       "iranian              0.877193\n",
       "psych-rock           0.877193\n",
       "punk                 0.877193\n",
       "afrobeat             0.877193\n",
       "r-n-b                0.877193\n",
       "turkish              0.877193\n",
       "trip-hop             0.877193\n",
       "trance               0.877193\n",
       "techno               0.877193\n",
       "tango                0.877193\n",
       "synth-pop            0.877193\n",
       "swedish              0.877193\n",
       "study                0.877193\n",
       "spanish              0.877193\n",
       "soul                 0.877193\n",
       "songwriter           0.877193\n",
       "sleep                0.877193\n",
       "ska                  0.877193\n",
       "singer-songwriter    0.877193\n",
       "show-tunes           0.877193\n",
       "sertanejo            0.877193\n",
       "samba                0.877193\n",
       "salsa                0.877193\n",
       "sad                  0.877193\n",
       "romance              0.877193\n",
       "rockabilly           0.877193\n",
       "rock                 0.877193\n",
       "rock-n-roll          0.877193\n",
       "reggaeton            0.877193\n",
       "reggae               0.877193\n",
       "industrial           0.877193\n",
       "indie                0.877193\n",
       "indie-pop            0.877193\n",
       "indian               0.877193\n",
       "disney               0.877193\n",
       "disco                0.877193\n",
       "detroit-techno       0.877193\n",
       "deep-house           0.877193\n",
       "death-metal          0.877193\n",
       "dancehall            0.877193\n",
       "dance                0.877193\n",
       "country              0.877193\n",
       "comedy               0.877193\n",
       "club                 0.877193\n",
       "classical            0.877193\n",
       "chill                0.877193\n",
       "children             0.877193\n",
       "chicago-house        0.877193\n",
       "cantopop             0.877193\n",
       "british              0.877193\n",
       "breakbeat            0.877193\n",
       "brazil               0.877193\n",
       "blues                0.877193\n",
       "bluegrass            0.877193\n",
       "black-metal          0.877193\n",
       "anime                0.877193\n",
       "ambient              0.877193\n",
       "alternative          0.877193\n",
       "alt-rock             0.877193\n",
       "drum-and-bass        0.877193\n",
       "dub                  0.877193\n",
       "dubstep              0.877193\n",
       "groove               0.877193\n",
       "idm                  0.877193\n",
       "house                0.877193\n",
       "honky-tonk           0.877193\n",
       "hip-hop              0.877193\n",
       "heavy-metal          0.877193\n",
       "hardstyle            0.877193\n",
       "hardcore             0.877193\n",
       "hard-rock            0.877193\n",
       "happy                0.877193\n",
       "guitar               0.877193\n",
       "grunge               0.877193\n",
       "grindcore            0.877193\n",
       "edm                  0.877193\n",
       "goth                 0.877193\n",
       "gospel               0.877193\n",
       "german               0.877193\n",
       "garage               0.877193\n",
       "funk                 0.877193\n",
       "french               0.877193\n",
       "forro                0.877193\n",
       "folk                 0.877193\n",
       "emo                  0.877193\n",
       "electronic           0.877193\n",
       "electro              0.877193\n",
       "world-music          0.877193\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.track_genre.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Drop trash columns](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114000 entries, 0 to 113999\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   track_id          114000 non-null  object \n",
      " 1   artists           113999 non-null  object \n",
      " 2   album_name        113999 non-null  object \n",
      " 3   track_name        113999 non-null  object \n",
      " 4   popularity        114000 non-null  int64  \n",
      " 5   duration_ms       114000 non-null  int64  \n",
      " 6   explicit          114000 non-null  bool   \n",
      " 7   danceability      114000 non-null  float64\n",
      " 8   energy            114000 non-null  float64\n",
      " 9   key               114000 non-null  int64  \n",
      " 10  loudness          114000 non-null  float64\n",
      " 11  mode              114000 non-null  int64  \n",
      " 12  speechiness       114000 non-null  float64\n",
      " 13  acousticness      114000 non-null  float64\n",
      " 14  instrumentalness  114000 non-null  float64\n",
      " 15  liveness          114000 non-null  float64\n",
      " 16  valence           114000 non-null  float64\n",
      " 17  tempo             114000 non-null  float64\n",
      " 18  time_signature    114000 non-null  int64  \n",
      " 19  track_genre       114000 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(5), object(5)\n",
      "memory usage: 16.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(df.columns[0], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Drop rows with missing values](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True, axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Selecting 50% of the data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57000, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.5, replace=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Removing Categorical Columns](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 57000 entries, 78611 to 13751\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   popularity        57000 non-null  int64  \n",
      " 1   duration_ms       57000 non-null  int64  \n",
      " 2   explicit          57000 non-null  bool   \n",
      " 3   energy            57000 non-null  float64\n",
      " 4   loudness          57000 non-null  float64\n",
      " 5   speechiness       57000 non-null  float64\n",
      " 6   acousticness      57000 non-null  float64\n",
      " 7   instrumentalness  57000 non-null  float64\n",
      " 8   liveness          57000 non-null  float64\n",
      " 9   valence           57000 non-null  float64\n",
      " 10  tempo             57000 non-null  float64\n",
      " 11  time_signature    57000 non-null  int64  \n",
      " 12  track_genre       57000 non-null  object \n",
      "dtypes: bool(1), float64(8), int64(3), object(1)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['track_id', 'artists', 'album_name', 'track_name','key', 'mode','danceability']\n",
    "df = df.drop(categorical_columns, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Split Train and Test Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(df, target_column, validation_size=0.1, test_size=0.1, random_state=42):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_column])\n",
    "    \n",
    "    df_train, df_validation = train_test_split(df_train,\n",
    "                                               test_size=validation_size/(1 - test_size),\n",
    "                                               random_state=random_state,\n",
    "                                               stratify=df_train[target_column])\n",
    "    return df_train, df_validation, df_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_validation, df_test = train_validation_test_split(df, \"track_genre\",0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 57000 entries, 78611 to 13751\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   popularity        57000 non-null  int64  \n",
      " 1   duration_ms       57000 non-null  int64  \n",
      " 2   explicit          57000 non-null  bool   \n",
      " 3   energy            57000 non-null  float64\n",
      " 4   loudness          57000 non-null  float64\n",
      " 5   speechiness       57000 non-null  float64\n",
      " 6   acousticness      57000 non-null  float64\n",
      " 7   instrumentalness  57000 non-null  float64\n",
      " 8   liveness          57000 non-null  float64\n",
      " 9   valence           57000 non-null  float64\n",
      " 10  tempo             57000 non-null  float64\n",
      " 11  time_signature    57000 non-null  int64  \n",
      " 12  track_genre       57000 non-null  object \n",
      "dtypes: bool(1), float64(8), int64(3), object(1)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Data Cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[Impute missing numeric data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "numeric_imputer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = numeric_imputer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = numeric_imputer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = numeric_imputer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Data Normalization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "\n",
    "normalizer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = normalizer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = normalizer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = normalizer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Model training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[KNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 euclidean 0.1792982456140351\n",
      "13 manhattan 0.2013157894736842\n",
      "17 euclidean 0.1782456140350877\n",
      "17 manhattan 0.20350877192982456\n",
      "21 euclidean 0.1812280701754386\n",
      "21 manhattan 0.20350877192982456\n",
      "25 euclidean 0.18078947368421053\n",
      "25 manhattan 0.20192982456140351\n",
      "29 euclidean 0.18043859649122806\n",
      "29 manhattan 0.19921052631578948\n",
      "33 euclidean 0.17842105263157895\n",
      "33 manhattan 0.20271929824561402\n",
      "37 euclidean 0.17868421052631578\n",
      "37 manhattan 0.20280701754385966\n",
      "41 euclidean 0.17982456140350878\n",
      "41 manhattan 0.20359649122807016\n",
      "45 euclidean 0.17850877192982456\n",
      "45 manhattan 0.2036842105263158\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': [13,17,21,25,29,33,37,41,45],#np.arange(10,50,1),\n",
    "    'metric': ['euclidean', 'manhattan']#, 'chebyshev' 'minkowski']\n",
    "}\n",
    "\n",
    "evaluation = {}\n",
    "\n",
    "eval_metrics = {\n",
    "    'precision':'precision_macro', \n",
    "    'recall':'recall_macro', \n",
    "    'f1':'f1_macro', \n",
    "    'accuracy':'accuracy'\n",
    "}\n",
    "\n",
    "for k in parameters['n_neighbors']:\n",
    "  for m in parameters['metric']:\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=m, n_jobs=-1)\n",
    "    scores = cross_validate(knn, df_train[numeric_columns], df_train['track_genre'], cv=5, scoring=eval_metrics, n_jobs=-1)\n",
    "    \n",
    "    knn.fit(df_train[numeric_columns], df_train['track_genre'])\n",
    "    y_pred_train = knn.predict(df_train[numeric_columns])\n",
    "    y_pred_valid = knn.predict(df_validation[numeric_columns])\n",
    "    \n",
    "    evaluation[(k, m)] = {\n",
    "        'cv_precision': scores['test_precision'].mean(),\n",
    "        'cv_recall': scores['test_recall'].mean(),\n",
    "        'cv_f1': scores['test_f1'].mean(),\n",
    "        'cv_accuracy': scores['test_accuracy'].mean(),\n",
    "        'train_f1': f1_score(df_train['track_genre'], y_pred_train, average='macro'),\n",
    "        'train_recall': recall_score(df_train['track_genre'], y_pred_train, average='macro'),\n",
    "        'train_precision': precision_score(df_train['track_genre'], y_pred_train, average='macro'),\n",
    "        'train_accuracy': accuracy_score(df_train['track_genre'], y_pred_train),\n",
    "        'validation_f1': f1_score(df_validation['track_genre'], y_pred_valid, average='macro'),\n",
    "        'validation_recall': recall_score(df_validation['track_genre'], y_pred_valid, average='macro'),\n",
    "        'validation_precision': precision_score(df_validation['track_genre'], y_pred_valid, average='macro'),\n",
    "        'validation_accuracy': accuracy_score(df_validation['track_genre'], y_pred_valid)\n",
    "    }\n",
    "    \n",
    "    print(k, m,accuracy_score(df_validation['track_genre'], y_pred_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_1_1_'></a>[Best model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 manhattan {'cv_precision': 0.19161525965470105, 'cv_recall': 0.20199923711840534, 'cv_f1': 0.18488288371108258, 'cv_accuracy': 0.20190058479532164, 'train_f1': 0.24698349069443096, 'train_recall': 0.2637649986801499, 'train_precision': 0.2615227656614058, 'train_accuracy': 0.26359649122807016, 'validation_f1': 0.18803048676768094, 'validation_recall': 0.20412055878098662, 'validation_precision': 0.1928877606201788, 'validation_accuracy': 0.2036842105263158}\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         acoustic       0.15      0.25      0.19       105\n",
      "         afrobeat       0.22      0.16      0.19       105\n",
      "         alt-rock       0.06      0.05      0.05       101\n",
      "      alternative       0.12      0.11      0.11       102\n",
      "          ambient       0.20      0.21      0.20        99\n",
      "            anime       0.08      0.07      0.07       100\n",
      "      black-metal       0.29      0.33      0.31       105\n",
      "        bluegrass       0.23      0.29      0.26        95\n",
      "            blues       0.07      0.04      0.05       104\n",
      "           brazil       0.05      0.05      0.05       104\n",
      "        breakbeat       0.28      0.21      0.24        99\n",
      "          british       0.02      0.01      0.01       100\n",
      "         cantopop       0.10      0.20      0.13        90\n",
      "    chicago-house       0.28      0.42      0.34       101\n",
      "         children       0.25      0.18      0.21        95\n",
      "            chill       0.14      0.15      0.14       101\n",
      "        classical       0.51      0.49      0.50       101\n",
      "             club       0.18      0.05      0.08       100\n",
      "           comedy       0.92      0.77      0.84       100\n",
      "          country       0.13      0.20      0.15       101\n",
      "            dance       0.11      0.20      0.14       101\n",
      "        dancehall       0.12      0.13      0.13       105\n",
      "      death-metal       0.20      0.26      0.23       101\n",
      "       deep-house       0.11      0.23      0.14       107\n",
      "   detroit-techno       0.35      0.54      0.43       101\n",
      "            disco       0.15      0.10      0.12       107\n",
      "           disney       0.29      0.08      0.13        98\n",
      "    drum-and-bass       0.37      0.52      0.43        95\n",
      "              dub       0.08      0.06      0.07       101\n",
      "          dubstep       0.12      0.21      0.15       100\n",
      "              edm       0.06      0.09      0.08        98\n",
      "          electro       0.07      0.09      0.08        92\n",
      "       electronic       0.09      0.01      0.02        99\n",
      "              emo       0.05      0.03      0.04       102\n",
      "             folk       0.07      0.03      0.04       101\n",
      "            forro       0.25      0.57      0.35       102\n",
      "           french       0.20      0.11      0.14        99\n",
      "             funk       0.20      0.13      0.16        97\n",
      "           garage       0.12      0.06      0.08       104\n",
      "           german       0.22      0.06      0.10        94\n",
      "           gospel       0.17      0.38      0.23        99\n",
      "             goth       0.09      0.03      0.04       100\n",
      "        grindcore       0.52      0.56      0.54        98\n",
      "           groove       0.09      0.03      0.05        97\n",
      "           grunge       0.07      0.09      0.08       100\n",
      "           guitar       0.28      0.21      0.24       102\n",
      "            happy       0.17      0.10      0.13        98\n",
      "        hard-rock       0.07      0.03      0.04       103\n",
      "         hardcore       0.17      0.13      0.15        97\n",
      "        hardstyle       0.34      0.45      0.39       102\n",
      "      heavy-metal       0.15      0.28      0.19        94\n",
      "          hip-hop       0.11      0.15      0.12       103\n",
      "       honky-tonk       0.40      0.64      0.49       101\n",
      "            house       0.10      0.14      0.12       101\n",
      "              idm       0.33      0.16      0.21       100\n",
      "           indian       0.06      0.06      0.06       100\n",
      "            indie       0.07      0.03      0.04       101\n",
      "        indie-pop       0.09      0.02      0.03       101\n",
      "       industrial       0.17      0.07      0.10        99\n",
      "          iranian       0.51      0.39      0.44       100\n",
      "          j-dance       0.38      0.44      0.41       101\n",
      "           j-idol       0.25      0.29      0.27        95\n",
      "            j-pop       0.03      0.02      0.02        98\n",
      "           j-rock       0.10      0.09      0.09       103\n",
      "             jazz       0.28      0.32      0.30        98\n",
      "            k-pop       0.07      0.06      0.07        96\n",
      "             kids       0.41      0.25      0.31        99\n",
      "            latin       0.13      0.23      0.17       105\n",
      "           latino       0.05      0.02      0.03        98\n",
      "            malay       0.16      0.10      0.12        97\n",
      "         mandopop       0.13      0.25      0.17       106\n",
      "            metal       0.10      0.10      0.10       105\n",
      "        metalcore       0.17      0.33      0.23        95\n",
      "   minimal-techno       0.27      0.45      0.33        96\n",
      "              mpb       0.12      0.13      0.12       102\n",
      "          new-age       0.36      0.51      0.42        99\n",
      "            opera       0.25      0.27      0.26        96\n",
      "           pagode       0.25      0.47      0.32        92\n",
      "            party       0.20      0.43      0.28       102\n",
      "            piano       0.36      0.36      0.36       107\n",
      "              pop       0.10      0.10      0.10       103\n",
      "         pop-film       0.14      0.31      0.19       101\n",
      "        power-pop       0.19      0.18      0.18        97\n",
      "progressive-house       0.13      0.22      0.16        97\n",
      "       psych-rock       0.13      0.07      0.09       104\n",
      "             punk       0.09      0.11      0.10        97\n",
      "        punk-rock       0.09      0.07      0.08        99\n",
      "            r-n-b       0.12      0.05      0.07        98\n",
      "           reggae       0.04      0.02      0.03       103\n",
      "        reggaeton       0.14      0.10      0.12        98\n",
      "             rock       0.11      0.08      0.09        96\n",
      "      rock-n-roll       0.23      0.18      0.20        96\n",
      "       rockabilly       0.33      0.08      0.13       101\n",
      "          romance       0.39      0.53      0.45       103\n",
      "              sad       0.15      0.09      0.11       101\n",
      "            salsa       0.31      0.54      0.40        96\n",
      "            samba       0.22      0.24      0.23       102\n",
      "        sertanejo       0.22      0.34      0.27        97\n",
      "       show-tunes       0.16      0.12      0.14       102\n",
      "singer-songwriter       0.07      0.04      0.05       101\n",
      "              ska       0.20      0.26      0.22        98\n",
      "            sleep       0.85      0.61      0.71       100\n",
      "       songwriter       0.08      0.03      0.04        98\n",
      "             soul       0.13      0.15      0.14       101\n",
      "          spanish       0.00      0.00      0.00       101\n",
      "            study       0.45      0.57      0.50       104\n",
      "          swedish       0.05      0.01      0.02       101\n",
      "        synth-pop       0.15      0.03      0.05        98\n",
      "            tango       0.46      0.59      0.52       100\n",
      "           techno       0.20      0.22      0.21       103\n",
      "           trance       0.34      0.35      0.34       104\n",
      "         trip-hop       0.20      0.03      0.05       100\n",
      "          turkish       0.17      0.15      0.16       101\n",
      "      world-music       0.16      0.20      0.18       101\n",
      "\n",
      "         accuracy                           0.21     11400\n",
      "        macro avg       0.20      0.21      0.19     11400\n",
      "     weighted avg       0.20      0.21      0.19     11400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(k, m,evaluation[(k,m)])\n",
    "\n",
    "best_knn = KNeighborsClassifier(n_neighbors=45, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "best_knn.fit(df_train[numeric_columns], df_train['track_genre'])\n",
    "\n",
    "print(classification_report(df_test['track_genre'], best_knn.predict(df_test[numeric_columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         acoustic       0.15      0.25      0.19       105\n",
      "         afrobeat       0.22      0.16      0.19       105\n",
      "         alt-rock       0.06      0.05      0.05       101\n",
      "      alternative       0.12      0.11      0.11       102\n",
      "          ambient       0.20      0.21      0.20        99\n",
      "            anime       0.08      0.07      0.07       100\n",
      "      black-metal       0.29      0.33      0.31       105\n",
      "        bluegrass       0.23      0.29      0.26        95\n",
      "            blues       0.07      0.04      0.05       104\n",
      "           brazil       0.05      0.05      0.05       104\n",
      "        breakbeat       0.28      0.21      0.24        99\n",
      "          british       0.02      0.01      0.01       100\n",
      "         cantopop       0.10      0.20      0.13        90\n",
      "    chicago-house       0.28      0.42      0.34       101\n",
      "         children       0.25      0.18      0.21        95\n",
      "            chill       0.14      0.15      0.14       101\n",
      "        classical       0.51      0.49      0.50       101\n",
      "             club       0.18      0.05      0.08       100\n",
      "           comedy       0.92      0.77      0.84       100\n",
      "          country       0.13      0.20      0.15       101\n",
      "            dance       0.11      0.20      0.14       101\n",
      "        dancehall       0.12      0.13      0.13       105\n",
      "      death-metal       0.20      0.26      0.23       101\n",
      "       deep-house       0.11      0.23      0.14       107\n",
      "   detroit-techno       0.35      0.54      0.43       101\n",
      "            disco       0.15      0.10      0.12       107\n",
      "           disney       0.29      0.08      0.13        98\n",
      "    drum-and-bass       0.37      0.52      0.43        95\n",
      "              dub       0.08      0.06      0.07       101\n",
      "          dubstep       0.12      0.21      0.15       100\n",
      "              edm       0.06      0.09      0.08        98\n",
      "          electro       0.07      0.09      0.08        92\n",
      "       electronic       0.09      0.01      0.02        99\n",
      "              emo       0.05      0.03      0.04       102\n",
      "             folk       0.07      0.03      0.04       101\n",
      "            forro       0.25      0.57      0.35       102\n",
      "           french       0.20      0.11      0.14        99\n",
      "             funk       0.20      0.13      0.16        97\n",
      "           garage       0.12      0.06      0.08       104\n",
      "           german       0.22      0.06      0.10        94\n",
      "           gospel       0.17      0.38      0.23        99\n",
      "             goth       0.09      0.03      0.04       100\n",
      "        grindcore       0.52      0.56      0.54        98\n",
      "           groove       0.09      0.03      0.05        97\n",
      "           grunge       0.07      0.09      0.08       100\n",
      "           guitar       0.28      0.21      0.24       102\n",
      "            happy       0.17      0.10      0.13        98\n",
      "        hard-rock       0.07      0.03      0.04       103\n",
      "         hardcore       0.17      0.13      0.15        97\n",
      "        hardstyle       0.34      0.45      0.39       102\n",
      "      heavy-metal       0.15      0.28      0.19        94\n",
      "          hip-hop       0.11      0.15      0.12       103\n",
      "       honky-tonk       0.40      0.64      0.49       101\n",
      "            house       0.10      0.14      0.12       101\n",
      "              idm       0.33      0.16      0.21       100\n",
      "           indian       0.06      0.06      0.06       100\n",
      "            indie       0.07      0.03      0.04       101\n",
      "        indie-pop       0.09      0.02      0.03       101\n",
      "       industrial       0.17      0.07      0.10        99\n",
      "          iranian       0.51      0.39      0.44       100\n",
      "          j-dance       0.38      0.44      0.41       101\n",
      "           j-idol       0.25      0.29      0.27        95\n",
      "            j-pop       0.03      0.02      0.02        98\n",
      "           j-rock       0.10      0.09      0.09       103\n",
      "             jazz       0.28      0.32      0.30        98\n",
      "            k-pop       0.07      0.06      0.07        96\n",
      "             kids       0.41      0.25      0.31        99\n",
      "            latin       0.13      0.23      0.17       105\n",
      "           latino       0.05      0.02      0.03        98\n",
      "            malay       0.16      0.10      0.12        97\n",
      "         mandopop       0.13      0.25      0.17       106\n",
      "            metal       0.10      0.10      0.10       105\n",
      "        metalcore       0.17      0.33      0.23        95\n",
      "   minimal-techno       0.27      0.45      0.33        96\n",
      "              mpb       0.12      0.13      0.12       102\n",
      "          new-age       0.36      0.51      0.42        99\n",
      "            opera       0.25      0.27      0.26        96\n",
      "           pagode       0.25      0.47      0.32        92\n",
      "            party       0.20      0.43      0.28       102\n",
      "            piano       0.36      0.36      0.36       107\n",
      "              pop       0.10      0.10      0.10       103\n",
      "         pop-film       0.14      0.31      0.19       101\n",
      "        power-pop       0.19      0.18      0.18        97\n",
      "progressive-house       0.13      0.22      0.16        97\n",
      "       psych-rock       0.13      0.07      0.09       104\n",
      "             punk       0.09      0.11      0.10        97\n",
      "        punk-rock       0.09      0.07      0.08        99\n",
      "            r-n-b       0.12      0.05      0.07        98\n",
      "           reggae       0.04      0.02      0.03       103\n",
      "        reggaeton       0.14      0.10      0.12        98\n",
      "             rock       0.11      0.08      0.09        96\n",
      "      rock-n-roll       0.23      0.18      0.20        96\n",
      "       rockabilly       0.33      0.08      0.13       101\n",
      "          romance       0.39      0.53      0.45       103\n",
      "              sad       0.15      0.09      0.11       101\n",
      "            salsa       0.31      0.54      0.40        96\n",
      "            samba       0.22      0.24      0.23       102\n",
      "        sertanejo       0.22      0.34      0.27        97\n",
      "       show-tunes       0.16      0.12      0.14       102\n",
      "singer-songwriter       0.07      0.04      0.05       101\n",
      "              ska       0.20      0.26      0.22        98\n",
      "            sleep       0.85      0.61      0.71       100\n",
      "       songwriter       0.08      0.03      0.04        98\n",
      "             soul       0.13      0.15      0.14       101\n",
      "          spanish       0.00      0.00      0.00       101\n",
      "            study       0.45      0.57      0.50       104\n",
      "          swedish       0.05      0.01      0.02       101\n",
      "        synth-pop       0.15      0.03      0.05        98\n",
      "            tango       0.46      0.59      0.52       100\n",
      "           techno       0.20      0.22      0.21       103\n",
      "           trance       0.34      0.35      0.34       104\n",
      "         trip-hop       0.20      0.03      0.05       100\n",
      "          turkish       0.17      0.15      0.16       101\n",
      "      world-music       0.16      0.20      0.18       101\n",
      "\n",
      "         accuracy                           0.21     11400\n",
      "        macro avg       0.20      0.21      0.19     11400\n",
      "     weighted avg       0.20      0.21      0.19     11400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_knn = KNeighborsClassifier(n_neighbors=45, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "best_knn.fit(df_train[numeric_columns], df_train['track_genre'])\n",
    "\n",
    "print(classification_report(df_test['track_genre'], best_knn.predict(df_test[numeric_columns])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_2_'></a>[LVQ](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "from math import sqrt\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "def calculate_metrics_per_class(actual, predicted, class_label):\n",
    "    TP = 0  # Verdadeiros Positivos\n",
    "    FP = 0  # Falsos Positivos\n",
    "    FN = 0  # Falsos Negativos\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i] == class_label:\n",
    "            TP += 1\n",
    "        elif predicted[i] == class_label and actual[i] != class_label:\n",
    "            FP += 1\n",
    "        elif actual[i] == class_label and predicted[i] != class_label:\n",
    "            FN += 1\n",
    "    return TP, FP, FN\n",
    "\n",
    "def macro_recall(actual, predicted):\n",
    "    unique_classes = set(actual)\n",
    "    recalls = []\n",
    "    for class_label in unique_classes:\n",
    "        TP, _, FN = calculate_metrics_per_class(actual, predicted, class_label)\n",
    "        recall = TP / (TP + FN) if (TP + FN) else 0\n",
    "        recalls.append(recall)\n",
    "    return sum(recalls) / len(recalls) * 100.0\n",
    "\n",
    "def macro_precision(actual, predicted):\n",
    "    unique_classes = set(actual)\n",
    "    precisions = []\n",
    "    for class_label in unique_classes:\n",
    "        TP, FP, _ = calculate_metrics_per_class(actual, predicted, class_label)\n",
    "        precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "        precisions.append(precision)\n",
    "    return sum(precisions) / len(precisions) * 100.0\n",
    "\n",
    "def macro_f1_score(actual, predicted):\n",
    "    precision = macro_precision(actual, predicted) / 100.0\n",
    "    recall = macro_recall(actual, predicted) / 100.0\n",
    "    return 2 * (precision * recall) / (precision + recall) * 100.0 if (precision + recall) else 0\n",
    "\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, dataset_validation, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores_cv = list()\n",
    "\tscores_val = list()\n",
    "\t\n",
    "\tscores_cv_recall = list()\n",
    "\tscores_cv_precision = list()\n",
    "\tscores_cv_f1 = list()\n",
    "\n",
    "\tscores_val_recall = list()\n",
    "\tscores_val_precision = list()\n",
    "\tscores_val_f1 = list()\n",
    "\n",
    "\n",
    "\tactual_validation = [row[-1] for row in dataset_validation]\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted_test, predicted_validation = algorithm(train_set, test_set, dataset_validation, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "  \n",
    "\t\taccuracy_test = accuracy_metric(actual, predicted_test)\n",
    "\t\tscores_cv.append(accuracy_test)\n",
    "  \n",
    "\t\tscores_cv_recall.append(macro_recall(actual, predicted_test))\n",
    "\t\tscores_cv_precision.append(macro_precision(actual, predicted_test))\n",
    "\t\tscores_cv_f1.append(macro_f1_score(actual, predicted_test))\n",
    "  \n",
    "\t\taccuracy_val = accuracy_metric(actual_validation, predicted_validation)\n",
    "\t\tscores_val.append(accuracy_val)\n",
    "\t\t\n",
    "\t\tscores_val_recall.append(macro_recall(actual, predicted_validation))\n",
    "\t\tscores_val_precision.append(macro_precision(actual, predicted_validation))\n",
    "\t\tscores_val_f1.append(macro_f1_score(actual, predicted_validation))\n",
    "\n",
    "\treturn scores_cv, scores_val, scores_cv_recall, scores_val_recall, scores_cv_precision, scores_val_precision, scores_cv_f1, scores_val_f1 \n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    "\n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "\tdistances = list()\n",
    "\tfor codebook in codebooks:\n",
    "\t\tdist = euclidean_distance(codebook, test_row)\n",
    "\t\tdistances.append((codebook, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\treturn distances[0][0]\n",
    "\n",
    "# Make a prediction with codebook vectors\n",
    "def predict(codebooks, test_row):\n",
    "\tbmu = get_best_matching_unit(codebooks, test_row)\n",
    "\treturn bmu[-1]\n",
    "\n",
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "\tn_records = len(train)\n",
    "\tn_features = len(train[0])\n",
    "\tcodebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "\treturn codebook\n",
    "\n",
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "\tfor epoch in range(epochs):\n",
    "\t\trate = lrate * (1.0-(epoch/float(epochs)))\n",
    "\t\tfor row in train:\n",
    "\t\t\tbmu = get_best_matching_unit(codebooks, row)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\terror = row[i] - bmu[i]\n",
    "\t\t\t\tif bmu[-1] == row[-1]:\n",
    "\t\t\t\t\tbmu[i] += rate * error\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbmu[i] -= rate * error\n",
    "\treturn codebooks\n",
    "\n",
    "# LVQ Algorithm\n",
    "def learning_vector_quantization(train, test, validation, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "\tpredictions_test = list()\n",
    "\tpredictions_validation = list()\n",
    "\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions_test.append(output)\n",
    "\tfor row in validation:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions_validation.append(output)\n",
    "  \n",
    "\treturn predictions_test, predictions_validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_train_2 = df_train.copy()\n",
    "df_train_2['track_genre'] = le.fit_transform(df_train_2['track_genre'])\n",
    "df_train_2['explicit'] = df_train_2['explicit'].astype(\"int\")\n",
    "\n",
    "df_validation_2 = df_validation.copy()\n",
    "df_validation_2['track_genre'] = le.transform(df_validation_2['track_genre'])\n",
    "df_validation_2['explicit'] = df_validation_2['explicit'].astype(\"int\")\n",
    "\n",
    "df_test_2 = df_test.copy()\n",
    "df_test_2['track_genre'] = le.transform(df_test_2['track_genre'])\n",
    "df_test_2['explicit'] = df_test_2['explicit'].astype(\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 40\n",
      "0.01 50\n",
      "0.01 60\n",
      "{(0.01, 40): {'cv_accuracy': 0.9005847953216375, 'validation_accuracy': 0.8666666666666666, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007899866625628398, 'validation_precision': 0.007899866625628398, 'cv_f1': 0.01565657078121029, 'validation_f1': 0.01565657078121029}, (0.01, 50): {'cv_accuracy': 0.9005847953216375, 'validation_accuracy': 0.8736842105263157, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007899866625628398, 'validation_precision': 0.007899866625628398, 'cv_f1': 0.01565451444004986, 'validation_f1': 0.01565451444004986}, (0.01, 60): {'cv_accuracy': 0.9122807017543859, 'validation_accuracy': 0.8754385964912281, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.008002462296091105, 'validation_precision': 0.008002462296091105, 'cv_f1': 0.015858577097302463, 'validation_f1': 0.015858577097302463}}\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Result too large')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearn_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     17\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_codebooks\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 19\u001b[0m     scores_cv, scores_vali, scores_cv_recall, scores_val_recall, scores_cv_precision, scores_val_precision, scores_cv_f1, scores_val_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_vector_quantization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     evaluation[(lr, n)] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(scores_cv)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scores_cv)),\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(scores_vali)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scores_vali)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28msum\u001b[39m(scores_val_f1)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scores_val_f1))\n\u001b[0;32m     31\u001b[0m     }\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(lr, n)\n",
      "Cell \u001b[1;32mIn[56], line 102\u001b[0m, in \u001b[0;36mevaluate_algorithm\u001b[1;34m(dataset, dataset_validation, algorithm, n_folds, *args)\u001b[0m\n\u001b[0;32m    100\u001b[0m \ttest_set\u001b[38;5;241m.\u001b[39mappend(row_copy)\n\u001b[0;32m    101\u001b[0m \trow_copy[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m predicted_test, predicted_validation \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m actual \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m fold]\n\u001b[0;32m    105\u001b[0m accuracy_test \u001b[38;5;241m=\u001b[39m accuracy_metric(actual, predicted_test)\n",
      "Cell \u001b[1;32mIn[56], line 166\u001b[0m, in \u001b[0;36mlearning_vector_quantization\u001b[1;34m(train, test, validation, n_codebooks, lrate, epochs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearning_vector_quantization\u001b[39m(train, test, validation, n_codebooks, lrate, epochs):\n\u001b[1;32m--> 166\u001b[0m \tcodebooks \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_codebooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_codebooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \tpredictions_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    168\u001b[0m \tpredictions_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "Cell \u001b[1;32mIn[56], line 155\u001b[0m, in \u001b[0;36mtrain_codebooks\u001b[1;34m(train, n_codebooks, lrate, epochs)\u001b[0m\n\u001b[0;32m    153\u001b[0m rate \u001b[38;5;241m=\u001b[39m lrate \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39m(epoch\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(epochs)))\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m train:\n\u001b[1;32m--> 155\u001b[0m \tbmu \u001b[38;5;241m=\u001b[39m \u001b[43mget_best_matching_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodebooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(row)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    157\u001b[0m \t\terror \u001b[38;5;241m=\u001b[39m row[i] \u001b[38;5;241m-\u001b[39m bmu[i]\n",
      "Cell \u001b[1;32mIn[56], line 132\u001b[0m, in \u001b[0;36mget_best_matching_unit\u001b[1;34m(codebooks, test_row)\u001b[0m\n\u001b[0;32m    130\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m codebook \u001b[38;5;129;01min\u001b[39;00m codebooks:\n\u001b[1;32m--> 132\u001b[0m \tdist \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \tdistances\u001b[38;5;241m.\u001b[39mappend((codebook, dist))\n\u001b[0;32m    134\u001b[0m distances\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m tup: tup[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[56], line 125\u001b[0m, in \u001b[0;36meuclidean_distance\u001b[1;34m(row1, row2)\u001b[0m\n\u001b[0;32m    123\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(row1)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m \tdistance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mrow1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(distance)\n",
      "\u001b[1;31mOverflowError\u001b[0m: (34, 'Result too large')"
     ]
    }
   ],
   "source": [
    "# Test LVQ on Spotify Dataset\n",
    "# load and prepare data\n",
    "n_folds = 5\n",
    "n_epochs = 20\n",
    "\n",
    "dataset_train = df_train_2.values.tolist()\n",
    "dataset_validation = df_validation_2.values.tolist()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'learn_rate': [0.01,0.2,0.5],\n",
    "    'n_codebooks': [40,50,60]\n",
    "}\n",
    "\n",
    "evaluation = {}\n",
    "for lr in parameters['learn_rate']:\n",
    "  for n in parameters['n_codebooks']:\n",
    "\n",
    "    scores_cv, scores_vali, scores_cv_recall, scores_val_recall, scores_cv_precision, scores_val_precision, scores_cv_f1, scores_val_f1 = evaluate_algorithm(\n",
    "      dataset_train, dataset_validation, learning_vector_quantization, n_folds, n, lr, n_epochs)\n",
    "\n",
    "    evaluation[(lr, n)] = {\n",
    "        'cv_accuracy': sum(scores_cv)/float(len(scores_cv)),\n",
    "        'validation_accuracy': sum(scores_vali)/float(len(scores_vali)),\n",
    "        'cv_recall': sum(scores_cv_recall)/float(len(scores_cv_recall)),\n",
    "        'validation_recall': sum(scores_val_recall)/float(len(scores_val_recall)),\n",
    "        'cv_precision': sum(scores_cv_precision)/float(len(scores_cv_precision)),\n",
    "        'validation_precision': sum(scores_val_precision)/float(len(scores_val_precision)),\n",
    "        'cv_f1': sum(scores_cv_f1)/float(len(scores_cv_f1)),\n",
    "        'validation_f1': sum(scores_val_f1)/float(len(scores_val_f1))\n",
    "    }\n",
    "    \n",
    "    print(lr, n)\n",
    "  print(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Result too large')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      7\u001b[0m n_codebooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m----> 9\u001b[0m scores_cv, scores_vali, scores_cv_recall, scores_val_recall, scores_cv_precision, scores_val_precision, scores_cv_f1, scores_val_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_vector_quantization\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_codebooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[44], line 102\u001b[0m, in \u001b[0;36mevaluate_algorithm\u001b[1;34m(dataset, dataset_validation, algorithm, n_folds, *args)\u001b[0m\n\u001b[0;32m    100\u001b[0m \ttest_set\u001b[38;5;241m.\u001b[39mappend(row_copy)\n\u001b[0;32m    101\u001b[0m \trow_copy[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m predicted_test, predicted_validation \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m actual \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m fold]\n\u001b[0;32m    105\u001b[0m accuracy_test \u001b[38;5;241m=\u001b[39m accuracy_metric(actual, predicted_test)\n",
      "Cell \u001b[1;32mIn[44], line 166\u001b[0m, in \u001b[0;36mlearning_vector_quantization\u001b[1;34m(train, test, validation, n_codebooks, lrate, epochs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearning_vector_quantization\u001b[39m(train, test, validation, n_codebooks, lrate, epochs):\n\u001b[1;32m--> 166\u001b[0m \tcodebooks \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_codebooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_codebooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \tpredictions_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    168\u001b[0m \tpredictions_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "Cell \u001b[1;32mIn[44], line 155\u001b[0m, in \u001b[0;36mtrain_codebooks\u001b[1;34m(train, n_codebooks, lrate, epochs)\u001b[0m\n\u001b[0;32m    153\u001b[0m rate \u001b[38;5;241m=\u001b[39m lrate \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39m(epoch\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(epochs)))\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m train:\n\u001b[1;32m--> 155\u001b[0m \tbmu \u001b[38;5;241m=\u001b[39m \u001b[43mget_best_matching_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodebooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(row)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    157\u001b[0m \t\terror \u001b[38;5;241m=\u001b[39m row[i] \u001b[38;5;241m-\u001b[39m bmu[i]\n",
      "Cell \u001b[1;32mIn[44], line 132\u001b[0m, in \u001b[0;36mget_best_matching_unit\u001b[1;34m(codebooks, test_row)\u001b[0m\n\u001b[0;32m    130\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m codebook \u001b[38;5;129;01min\u001b[39;00m codebooks:\n\u001b[1;32m--> 132\u001b[0m \tdist \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \tdistances\u001b[38;5;241m.\u001b[39mappend((codebook, dist))\n\u001b[0;32m    134\u001b[0m distances\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m tup: tup[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[44], line 125\u001b[0m, in \u001b[0;36meuclidean_distance\u001b[1;34m(row1, row2)\u001b[0m\n\u001b[0;32m    123\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(row1)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m \tdistance \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mrow1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sqrt(distance)\n",
      "\u001b[1;31mOverflowError\u001b[0m: (34, 'Result too large')"
     ]
    }
   ],
   "source": [
    "dataset_train = df_train_2.values.tolist()\n",
    "dataset_test = df_test_2.values.tolist()\n",
    "\n",
    "n_folds = 5\n",
    "learn_rate = 0.1\n",
    "n_epochs = 20\n",
    "n_codebooks = 30\n",
    "\n",
    "scores_cv, scores_vali, scores_cv_recall, scores_val_recall, scores_cv_precision, scores_val_precision, scores_cv_f1, scores_val_f1 = evaluate_algorithm(\n",
    "    dataset_train, dataset_test, learning_vector_quantization,n_folds, n_codebooks, learn_rate, n_epochs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
