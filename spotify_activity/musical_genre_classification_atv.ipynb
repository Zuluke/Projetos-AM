{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Imports](#toc1_)    \n",
    "- [Read Data](#toc2_)    \n",
    "- [Data Selection](#toc3_)    \n",
    "  - [Selecting 50% of the data](#toc3_3_)\n",
    "  - [Drop rows with missing values](#toc3_2_)    \n",
    "- [Removing Categorical Columns](#toc4_)    \n",
    "- [Split Train and Test Data](#toc5_)    \n",
    "- [Data Cleaning](#toc6_)    \n",
    "  - [Impute missing numeric data](#toc6_1_)    \n",
    "- [Data Normalization](#toc7_)    \n",
    "- [Model training](#toc8_)    \n",
    "  - [KNN](#toc8_1_)    \n",
    "    - [Best model](#toc8_1_1_)    \n",
    "  - [LVQ](#toc8_2_)\n",
    "  - [Decision Tree](#toc8_3_)  \n",
    "  - [MLP](#toc8_4_)\n",
    "  - [SVM](#toc8_5_)  \n",
    "  - [Stacking](#toc8_6_)  \n",
    "  - [Random Forest](#toc8_7_)  \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_validate,train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, classification_report\n",
    "import random\n",
    "from random import seed,randrange\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Read Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the csv file from your GitHub account\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Zuluke/Projetos-AM/main/spotify_activity/dataset.csv\" # Make sure the url is the raw version of the file on GitHub\n",
    "download = requests.get(url).content\n",
    "\n",
    "# Reading the downloaded content and turning it into a pandas dataframe\n",
    "\n",
    "dataset = pd.read_csv(io.StringIO(download.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Visualize Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114000, 21)\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114000 entries, 0 to 113999\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Unnamed: 0        114000 non-null  int64  \n",
      " 1   track_id          114000 non-null  object \n",
      " 2   artists           113999 non-null  object \n",
      " 3   album_name        113999 non-null  object \n",
      " 4   track_name        113999 non-null  object \n",
      " 5   popularity        114000 non-null  int64  \n",
      " 6   duration_ms       114000 non-null  int64  \n",
      " 7   explicit          114000 non-null  bool   \n",
      " 8   danceability      114000 non-null  float64\n",
      " 9   energy            114000 non-null  float64\n",
      " 10  key               114000 non-null  int64  \n",
      " 11  loudness          114000 non-null  float64\n",
      " 12  mode              114000 non-null  int64  \n",
      " 13  speechiness       114000 non-null  float64\n",
      " 14  acousticness      114000 non-null  float64\n",
      " 15  instrumentalness  114000 non-null  float64\n",
      " 16  liveness          114000 non-null  float64\n",
      " 17  valence           114000 non-null  float64\n",
      " 18  tempo             114000 non-null  float64\n",
      " 19  time_signature    114000 non-null  int64  \n",
      " 20  track_genre       114000 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(6), object(5)\n",
      "memory usage: 17.5+ MB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Soundtrack)</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                track_id                 artists  \\\n",
       "0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
       "1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "\n",
       "                                               album_name  \\\n",
       "0                                                  Comedy   \n",
       "1                                        Ghost (Acoustic)   \n",
       "2                                          To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Soundtrack)   \n",
       "4                                                 Hold On   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "0                      Comedy          73       230666     False   \n",
       "1            Ghost - Acoustic          55       149610     False   \n",
       "2              To Begin Again          57       210826     False   \n",
       "3  Can't Help Falling In Love          71       201933     False   \n",
       "4                     Hold On          82       198853     False   \n",
       "\n",
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
       "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
       "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
       "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
       "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
       "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
       "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
       "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
       "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
       "4          0.000000    0.0829    0.167  119.949               4    acoustic  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape\n",
    "print(dataset.shape)\n",
    "print('\\n')\n",
    "dataset.info()\n",
    "print('\\n')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data Selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Selecting 50% of the data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_genre\n",
       "techno               0.877193\n",
       "edm                  0.877193\n",
       "disney               0.877193\n",
       "samba                0.877193\n",
       "latino               0.877193\n",
       "turkish              0.877193\n",
       "pagode               0.877193\n",
       "mpb                  0.877193\n",
       "piano                0.877193\n",
       "electro              0.877193\n",
       "metal                0.877193\n",
       "sleep                0.877193\n",
       "afrobeat             0.877193\n",
       "alternative          0.877193\n",
       "grindcore            0.877193\n",
       "industrial           0.877193\n",
       "club                 0.877193\n",
       "drum-and-bass        0.877193\n",
       "malay                0.877193\n",
       "ambient              0.877193\n",
       "rockabilly           0.877193\n",
       "death-metal          0.877193\n",
       "synth-pop            0.877193\n",
       "dance                0.877193\n",
       "goth                 0.877193\n",
       "brazil               0.877193\n",
       "tango                0.877193\n",
       "groove               0.877193\n",
       "sertanejo            0.877193\n",
       "punk                 0.877193\n",
       "deep-house           0.877193\n",
       "detroit-techno       0.877193\n",
       "minimal-techno       0.877193\n",
       "folk                 0.877193\n",
       "latin                0.877193\n",
       "ska                  0.877193\n",
       "j-dance              0.877193\n",
       "indian               0.877193\n",
       "sad                  0.877193\n",
       "german               0.877193\n",
       "dubstep              0.877193\n",
       "j-pop                0.877193\n",
       "emo                  0.877193\n",
       "honky-tonk           0.877193\n",
       "new-age              0.877193\n",
       "comedy               0.877193\n",
       "study                0.877193\n",
       "idm                  0.877193\n",
       "trance               0.877193\n",
       "indie-pop            0.877193\n",
       "forro                0.877193\n",
       "reggae               0.877193\n",
       "house                0.877193\n",
       "j-idol               0.877193\n",
       "r-n-b                0.877193\n",
       "pop-film             0.877193\n",
       "iranian              0.877193\n",
       "kids                 0.877193\n",
       "heavy-metal          0.877193\n",
       "trip-hop             0.877193\n",
       "spanish              0.877193\n",
       "mandopop             0.877193\n",
       "anime                0.877193\n",
       "pop                  0.877193\n",
       "country              0.877193\n",
       "electronic           0.877193\n",
       "alt-rock             0.877193\n",
       "chill                0.877193\n",
       "acoustic             0.877193\n",
       "indie                0.877193\n",
       "power-pop            0.877193\n",
       "garage               0.877193\n",
       "british              0.877193\n",
       "reggaeton            0.877193\n",
       "show-tunes           0.877193\n",
       "opera                0.877193\n",
       "psych-rock           0.877193\n",
       "songwriter           0.877193\n",
       "bluegrass            0.877193\n",
       "happy                0.877193\n",
       "punk-rock            0.877193\n",
       "dancehall            0.877193\n",
       "k-pop                0.877193\n",
       "disco                0.877193\n",
       "party                0.877193\n",
       "rock                 0.877193\n",
       "cantopop             0.877193\n",
       "hardstyle            0.877193\n",
       "metalcore            0.877193\n",
       "hard-rock            0.877193\n",
       "romance              0.877193\n",
       "progressive-house    0.877193\n",
       "j-rock               0.877193\n",
       "chicago-house        0.877193\n",
       "classical            0.877193\n",
       "swedish              0.877193\n",
       "salsa                0.877193\n",
       "gospel               0.877193\n",
       "soul                 0.877193\n",
       "dub                  0.877193\n",
       "singer-songwriter    0.877193\n",
       "rock-n-roll          0.877193\n",
       "black-metal          0.877193\n",
       "french               0.877193\n",
       "hip-hop              0.877193\n",
       "guitar               0.877193\n",
       "funk                 0.877193\n",
       "jazz                 0.877193\n",
       "breakbeat            0.877193\n",
       "grunge               0.877193\n",
       "hardcore             0.877193\n",
       "children             0.877193\n",
       "world-music          0.877193\n",
       "blues                0.877193\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, _, = train_test_split(dataset, train_size=0.5, stratify=dataset.track_genre)\n",
    "df.track_genre.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Drop rows with missing values](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True, axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Removing Categorical Columns](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 57000 entries, 109074 to 40999\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   popularity        57000 non-null  int64  \n",
      " 1   duration_ms       57000 non-null  int64  \n",
      " 2   danceability      57000 non-null  float64\n",
      " 3   energy            57000 non-null  float64\n",
      " 4   loudness          57000 non-null  float64\n",
      " 5   speechiness       57000 non-null  float64\n",
      " 6   acousticness      57000 non-null  float64\n",
      " 7   instrumentalness  57000 non-null  float64\n",
      " 8   liveness          57000 non-null  float64\n",
      " 9   valence           57000 non-null  float64\n",
      " 10  tempo             57000 non-null  float64\n",
      " 11  track_genre       57000 non-null  object \n",
      "dtypes: float64(9), int64(2), object(1)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', 'explicit', 'key', 'mode', 'time_signature']\n",
    "df = df.drop(categorical_columns, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Split Train and Test Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(df, target_column, validation_size=0.1, test_size=0.1, random_state=42):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_column])\n",
    "    \n",
    "    df_train, df_validation = train_test_split(df_train,\n",
    "                                               test_size=validation_size/(1 - test_size),\n",
    "                                               random_state=random_state,\n",
    "                                               stratify=df_train[target_column])\n",
    "    return df_train, df_validation, df_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 57000 entries, 109074 to 40999\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   popularity        57000 non-null  int64  \n",
      " 1   duration_ms       57000 non-null  int64  \n",
      " 2   danceability      57000 non-null  float64\n",
      " 3   energy            57000 non-null  float64\n",
      " 4   loudness          57000 non-null  float64\n",
      " 5   speechiness       57000 non-null  float64\n",
      " 6   acousticness      57000 non-null  float64\n",
      " 7   instrumentalness  57000 non-null  float64\n",
      " 8   liveness          57000 non-null  float64\n",
      " 9   valence           57000 non-null  float64\n",
      " 10  tempo             57000 non-null  float64\n",
      " 11  track_genre       57000 non-null  object \n",
      "dtypes: float64(9), int64(2), object(1)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train, df_validation, df_test = train_validation_test_split(df, \"track_genre\",0.2, 0.2)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Data Cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[Impute missing numeric data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "numeric_imputer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = numeric_imputer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = numeric_imputer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = numeric_imputer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Data Normalization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "\n",
    "normalizer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = normalizer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = normalizer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = normalizer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Model training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[KNN Prof](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distância euclidean:  32%|███▎      | 13/40 [00:19<00:42,  1.58s/it]"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fazendo uma cópia de amostra do conjunto de dados\n",
    "df_aj, _, = train_test_split(dataset, train_size=0.5, stratify=dataset.track_genre)\n",
    "\n",
    "# Removendo colunas categóricas nominais e de palavras de baixo calão\n",
    "df_aj = df_aj.drop(columns=['Unnamed: 0','track_id','energy','speechiness','acousticness','instrumentalness','liveness',\n",
    "                                      'duration_ms','explicit' ,'artists', 'popularity','loudness','time_signature',\n",
    "                                      'album_name', 'tempo','track_name','key', 'mode','danceability'])\n",
    "df_aj = df_aj\n",
    "\n",
    "df_aj.dropna(inplace=True, axis=0, how='any')\n",
    "\n",
    "# Criando dicionários para converter labels em números e vice-versa\n",
    "dict_label_num = {}\n",
    "dict_num_label = {}\n",
    "for index in range(len(df_aj.track_genre.unique())):\n",
    "  dict_label_num[df_aj.track_genre.unique()[index]] = index\n",
    "  dict_num_label[index] = df_aj.track_genre.unique()[index]\n",
    "\n",
    "# Renomeando valores de track_genre\n",
    "df_aj.track_genre = df_aj.track_genre.map(dict_label_num)\n",
    "\n",
    "# Divisão de dados de atributos e classe\n",
    "sptf_X = df_aj.drop(columns='track_genre') #caracteristicas\n",
    "sptf_Y = df_aj.track_genre #classe\n",
    "\n",
    "# Divisão em conjuntos de treino, teste e validação\n",
    "sptf_X_train, sptf_X_test, sptf_Y_train, sptf_Y_test = train_test_split(sptf_X, sptf_Y, test_size=0.40, random_state=10)\n",
    "sptf_X_test, sptf_X_valid, sptf_Y_test, sptf_Y_valid = train_test_split(sptf_X_test, sptf_Y_test, test_size=0.50, random_state=10)\n",
    "\n",
    "sptf_X_train = sptf_X_train.values\n",
    "sptf_X_test = sptf_X_test.values\n",
    "sptf_X_valid = sptf_X_valid.values\n",
    "sptf_Y_train = sptf_Y_train.values\n",
    "sptf_Y_test = sptf_Y_test.values\n",
    "sptf_Y_valid = sptf_Y_valid.values\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "sptf_clf = neighbors.KNeighborsClassifier()\n",
    "\n",
    "# Construindo o espaco de busca por configuracoes do classificador\n",
    "k_range = range(1, 81, 2) #k\n",
    "k_scores_train = []\n",
    "k_scores_train_full = []\n",
    "k_scores_valid = []\n",
    "vet_distancias = ['euclidean', 'manhattan']\n",
    "best_f1 = 0\n",
    "#p_range = range(1, 198) #k\n",
    "# use iteration to caclulator different k in models, then return the average accuracy based on the cross validation\n",
    "for k in vet_distancias:\n",
    "  for j in tqdm(k_range, desc=f'Distância {k}'):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=j, metric=k)\n",
    "    scores = cross_val_score(knn, sptf_X_train, sptf_Y_train, cv=5, scoring='f1_weighted')\n",
    "    k_scores_train.append(scores.mean())\n",
    "    if scores.mean() > best_f1:\n",
    "      sptf_clf = neighbors.KNeighborsClassifier(n_neighbors=j, metric=k)\n",
    "      best_f1 = scores.mean()\n",
    "    knn.fit(sptf_X_train, sptf_Y_train)\n",
    "    k_scores_train_full.append(f1_score(sptf_Y_train, knn.predict(sptf_X_train), average='weighted'))\n",
    "    k_scores_valid.append(f1_score(sptf_Y_valid, knn.predict(sptf_X_valid), average='weighted'))\n",
    "\n",
    "#treinando o classificador\n",
    "sptf_clf = sptf_clf.fit(sptf_X_train, sptf_Y_train)\n",
    "\n",
    "# plot to see clearly\n",
    "plt.plot(list(range(0,len(k_scores_train))), k_scores_train)\n",
    "plt.plot(list(range(0,len(k_scores_train_full))), k_scores_train_full)\n",
    "plt.plot(list(range(0,len(k_scores_valid))), k_scores_valid)\n",
    "plt.legend(('Score medio Treino CV', 'Conj. Treino', 'Conj. Validacao'),\n",
    "           loc='upper right', shadow=True)\n",
    "plt.xlabel('Values of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.show()\n",
    "\n",
    "print(\"F1 de treinamento clf: %0.3f\" %  f1_score(sptf_Y_train, sptf_clf.predict(sptf_X_train), average='weighted'))\n",
    "print(\"F1 de validação clf: %0.3f\" %  f1_score(sptf_Y_valid, sptf_clf.predict(sptf_X_valid), average='weighted'))\n",
    "print(\"F1 de teste clf: %0.3f\" %  f1_score(sptf_Y_test, sptf_clf.predict(sptf_X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[KNN Veva](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 euclidean 0.19929824561403509\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_validate(knn, df_train[numeric_columns], df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_genre\u001b[39m\u001b[38;5;124m'\u001b[39m], cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39meval_metrics, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(df_train[numeric_columns], df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_genre\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 24\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(df_train[numeric_columns])\n\u001b[0;32m     25\u001b[0m y_pred_valid \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(df_validation[numeric_columns])\n\u001b[0;32m     27\u001b[0m evaluation[(k, m)] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_precision\u001b[39m\u001b[38;5;124m'\u001b[39m: scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_precision\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_recall\u001b[39m\u001b[38;5;124m'\u001b[39m: scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_recall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(df_validation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_genre\u001b[39m\u001b[38;5;124m'\u001b[39m], y_pred_valid)\n\u001b[0;32m     40\u001b[0m }\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:234\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    Class labels for each data sample.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    235\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:879\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    874\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    875\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    876\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    877\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method\n\u001b[0;32m    878\u001b[0m         )\n\u001b[1;32m--> 879\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m    880\u001b[0m         delayed(_tree_query_parallel_helper)(\n\u001b[0;32m    881\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree, X[s], n_neighbors, return_distance\n\u001b[0;32m    882\u001b[0m         )\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m gen_even_slices(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_jobs)\n\u001b[0;32m    884\u001b[0m     )\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal: _fit_method not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\multiprocessing\\pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[0;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\multiprocessing\\pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_neighbors': np.arange(1,50,5),\n",
    "    'metric': ['euclidean', 'manhattan']#, 'chebyshev' 'minkowski']\n",
    "}\n",
    "\n",
    "evaluation = {}\n",
    "\n",
    "eval_metrics = {\n",
    "    'precision':'precision_macro', \n",
    "    'recall':'recall_macro', \n",
    "    'f1':'f1_macro', \n",
    "    'accuracy':'accuracy'\n",
    "}\n",
    "\n",
    "for k in parameters['n_neighbors']:\n",
    "  for m in parameters['metric']:\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=m, n_jobs=-1)\n",
    "    scores = cross_validate(knn, df_train[numeric_columns], df_train['track_genre'], cv=5, scoring=eval_metrics, n_jobs=-1)\n",
    "    \n",
    "    knn.fit(df_train[numeric_columns], df_train['track_genre'])\n",
    "    y_pred_train = knn.predict(df_train[numeric_columns])\n",
    "    y_pred_valid = knn.predict(df_validation[numeric_columns])\n",
    "    \n",
    "    evaluation[(k, m)] = {\n",
    "        'cv_precision': scores['test_precision'].mean(),\n",
    "        'cv_recall': scores['test_recall'].mean(),\n",
    "        'cv_f1': scores['test_f1'].mean(),\n",
    "        'cv_accuracy': scores['test_accuracy'].mean(),\n",
    "        'train_f1': f1_score(df_train['track_genre'], y_pred_train, average='macro'),\n",
    "        'train_recall': recall_score(df_train['track_genre'], y_pred_train, average='macro'),\n",
    "        'train_precision': precision_score(df_train['track_genre'], y_pred_train, average='macro'),\n",
    "        'train_accuracy': accuracy_score(df_train['track_genre'], y_pred_train),\n",
    "        'validation_f1': f1_score(df_validation['track_genre'], y_pred_valid, average='macro'),\n",
    "        'validation_recall': recall_score(df_validation['track_genre'], y_pred_valid, average='macro'),\n",
    "        'validation_precision': precision_score(df_validation['track_genre'], y_pred_valid, average='macro'),\n",
    "        'validation_accuracy': accuracy_score(df_validation['track_genre'], y_pred_valid)\n",
    "    }\n",
    "    \n",
    "    print(k, m,accuracy_score(df_validation['track_genre'], y_pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Div. de dados atributos e classe\n",
    "df_cara_train = df_train[numeric_columns].values  #caracteristicas\n",
    "df_clas_train = df_train['track_genre'].values #classe\n",
    "\n",
    "df_cara_validation = df_validation[numeric_columns].values  #caracteristicas\n",
    "df_clas_validation = df_validation['track_genre'].values #classe\n",
    "\n",
    "df_cara_test = df_test[numeric_columns].values  #caracteristicas\n",
    "df_clas_test = df_test['track_genre'].values #classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier().fit(df_cara_train,df_clas_train)\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1,50,5),\n",
    "    'metric': ['euclidean', 'manhattan']#, 'chebyshev' 'minkowski']\n",
    "}\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(df_cara_train,df_clas_train)\n",
    "print(grid.cv_results_['mean_test_score'],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_pred = grid.best_estimator_.predict(df_cara_test)\n",
    "\n",
    "evaluation={\n",
    "'accuracy': accuracy_score(df_clas_test, df_clas_pred),\n",
    "'precision': precision_score(df_clas_test, df_clas_pred, average='weighted'),\n",
    "'recall': recall_score(df_clas_test, df_clas_pred, average='weighted'),\n",
    "'f1': f1_score(df_clas_test, df_clas_pred, average='weighted')\n",
    "}\n",
    "\n",
    "print(f'Dados de Teste')\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_1_1_'></a>[Best model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 manhattan {'cv_precision': 0.19161525965470105, 'cv_recall': 0.20199923711840534, 'cv_f1': 0.18488288371108258, 'cv_accuracy': 0.20190058479532164, 'train_f1': 0.24698349069443096, 'train_recall': 0.2637649986801499, 'train_precision': 0.2615227656614058, 'train_accuracy': 0.26359649122807016, 'validation_f1': 0.18803048676768094, 'validation_recall': 0.20412055878098662, 'validation_precision': 0.1928877606201788, 'validation_accuracy': 0.2036842105263158}\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         acoustic       0.15      0.25      0.19       105\n",
      "         afrobeat       0.22      0.16      0.19       105\n",
      "         alt-rock       0.06      0.05      0.05       101\n",
      "      alternative       0.12      0.11      0.11       102\n",
      "          ambient       0.20      0.21      0.20        99\n",
      "            anime       0.08      0.07      0.07       100\n",
      "      black-metal       0.29      0.33      0.31       105\n",
      "        bluegrass       0.23      0.29      0.26        95\n",
      "            blues       0.07      0.04      0.05       104\n",
      "           brazil       0.05      0.05      0.05       104\n",
      "        breakbeat       0.28      0.21      0.24        99\n",
      "          british       0.02      0.01      0.01       100\n",
      "         cantopop       0.10      0.20      0.13        90\n",
      "    chicago-house       0.28      0.42      0.34       101\n",
      "         children       0.25      0.18      0.21        95\n",
      "            chill       0.14      0.15      0.14       101\n",
      "        classical       0.51      0.49      0.50       101\n",
      "             club       0.18      0.05      0.08       100\n",
      "           comedy       0.92      0.77      0.84       100\n",
      "          country       0.13      0.20      0.15       101\n",
      "            dance       0.11      0.20      0.14       101\n",
      "        dancehall       0.12      0.13      0.13       105\n",
      "      death-metal       0.20      0.26      0.23       101\n",
      "       deep-house       0.11      0.23      0.14       107\n",
      "   detroit-techno       0.35      0.54      0.43       101\n",
      "            disco       0.15      0.10      0.12       107\n",
      "           disney       0.29      0.08      0.13        98\n",
      "    drum-and-bass       0.37      0.52      0.43        95\n",
      "              dub       0.08      0.06      0.07       101\n",
      "          dubstep       0.12      0.21      0.15       100\n",
      "              edm       0.06      0.09      0.08        98\n",
      "          electro       0.07      0.09      0.08        92\n",
      "       electronic       0.09      0.01      0.02        99\n",
      "              emo       0.05      0.03      0.04       102\n",
      "             folk       0.07      0.03      0.04       101\n",
      "            forro       0.25      0.57      0.35       102\n",
      "           french       0.20      0.11      0.14        99\n",
      "             funk       0.20      0.13      0.16        97\n",
      "           garage       0.12      0.06      0.08       104\n",
      "           german       0.22      0.06      0.10        94\n",
      "           gospel       0.17      0.38      0.23        99\n",
      "             goth       0.09      0.03      0.04       100\n",
      "        grindcore       0.52      0.56      0.54        98\n",
      "           groove       0.09      0.03      0.05        97\n",
      "           grunge       0.07      0.09      0.08       100\n",
      "           guitar       0.28      0.21      0.24       102\n",
      "            happy       0.17      0.10      0.13        98\n",
      "        hard-rock       0.07      0.03      0.04       103\n",
      "         hardcore       0.17      0.13      0.15        97\n",
      "        hardstyle       0.34      0.45      0.39       102\n",
      "      heavy-metal       0.15      0.28      0.19        94\n",
      "          hip-hop       0.11      0.15      0.12       103\n",
      "       honky-tonk       0.40      0.64      0.49       101\n",
      "            house       0.10      0.14      0.12       101\n",
      "              idm       0.33      0.16      0.21       100\n",
      "           indian       0.06      0.06      0.06       100\n",
      "            indie       0.07      0.03      0.04       101\n",
      "        indie-pop       0.09      0.02      0.03       101\n",
      "       industrial       0.17      0.07      0.10        99\n",
      "          iranian       0.51      0.39      0.44       100\n",
      "          j-dance       0.38      0.44      0.41       101\n",
      "           j-idol       0.25      0.29      0.27        95\n",
      "            j-pop       0.03      0.02      0.02        98\n",
      "           j-rock       0.10      0.09      0.09       103\n",
      "             jazz       0.28      0.32      0.30        98\n",
      "            k-pop       0.07      0.06      0.07        96\n",
      "             kids       0.41      0.25      0.31        99\n",
      "            latin       0.13      0.23      0.17       105\n",
      "           latino       0.05      0.02      0.03        98\n",
      "            malay       0.16      0.10      0.12        97\n",
      "         mandopop       0.13      0.25      0.17       106\n",
      "            metal       0.10      0.10      0.10       105\n",
      "        metalcore       0.17      0.33      0.23        95\n",
      "   minimal-techno       0.27      0.45      0.33        96\n",
      "              mpb       0.12      0.13      0.12       102\n",
      "          new-age       0.36      0.51      0.42        99\n",
      "            opera       0.25      0.27      0.26        96\n",
      "           pagode       0.25      0.47      0.32        92\n",
      "            party       0.20      0.43      0.28       102\n",
      "            piano       0.36      0.36      0.36       107\n",
      "              pop       0.10      0.10      0.10       103\n",
      "         pop-film       0.14      0.31      0.19       101\n",
      "        power-pop       0.19      0.18      0.18        97\n",
      "progressive-house       0.13      0.22      0.16        97\n",
      "       psych-rock       0.13      0.07      0.09       104\n",
      "             punk       0.09      0.11      0.10        97\n",
      "        punk-rock       0.09      0.07      0.08        99\n",
      "            r-n-b       0.12      0.05      0.07        98\n",
      "           reggae       0.04      0.02      0.03       103\n",
      "        reggaeton       0.14      0.10      0.12        98\n",
      "             rock       0.11      0.08      0.09        96\n",
      "      rock-n-roll       0.23      0.18      0.20        96\n",
      "       rockabilly       0.33      0.08      0.13       101\n",
      "          romance       0.39      0.53      0.45       103\n",
      "              sad       0.15      0.09      0.11       101\n",
      "            salsa       0.31      0.54      0.40        96\n",
      "            samba       0.22      0.24      0.23       102\n",
      "        sertanejo       0.22      0.34      0.27        97\n",
      "       show-tunes       0.16      0.12      0.14       102\n",
      "singer-songwriter       0.07      0.04      0.05       101\n",
      "              ska       0.20      0.26      0.22        98\n",
      "            sleep       0.85      0.61      0.71       100\n",
      "       songwriter       0.08      0.03      0.04        98\n",
      "             soul       0.13      0.15      0.14       101\n",
      "          spanish       0.00      0.00      0.00       101\n",
      "            study       0.45      0.57      0.50       104\n",
      "          swedish       0.05      0.01      0.02       101\n",
      "        synth-pop       0.15      0.03      0.05        98\n",
      "            tango       0.46      0.59      0.52       100\n",
      "           techno       0.20      0.22      0.21       103\n",
      "           trance       0.34      0.35      0.34       104\n",
      "         trip-hop       0.20      0.03      0.05       100\n",
      "          turkish       0.17      0.15      0.16       101\n",
      "      world-music       0.16      0.20      0.18       101\n",
      "\n",
      "         accuracy                           0.21     11400\n",
      "        macro avg       0.20      0.21      0.19     11400\n",
      "     weighted avg       0.20      0.21      0.19     11400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(k, m,evaluation[(k,m)])\n",
    "\n",
    "best_knn = KNeighborsClassifier(n_neighbors=45, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "best_knn.fit(df_train[numeric_columns], df_train['track_genre'])\n",
    "\n",
    "print(classification_report(df_test['track_genre'], best_knn.predict(df_test[numeric_columns])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_2_'></a>[LVQ](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "def calculate_metrics_per_class(actual, predicted, class_label):\n",
    "    TP = 0  # Verdadeiros Positivos\n",
    "    FP = 0  # Falsos Positivos\n",
    "    FN = 0  # Falsos Negativos\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i] == class_label:\n",
    "            TP += 1\n",
    "        elif predicted[i] == class_label and actual[i] != class_label:\n",
    "            FP += 1\n",
    "        elif actual[i] == class_label and predicted[i] != class_label:\n",
    "            FN += 1\n",
    "    return TP, FP, FN\n",
    "\n",
    "def macro_recall(actual, predicted):\n",
    "    unique_classes = set(actual)\n",
    "    recalls = []\n",
    "    for class_label in unique_classes:\n",
    "        TP, _, FN = calculate_metrics_per_class(actual, predicted, class_label)\n",
    "        recall = TP / (TP + FN) if (TP + FN) else 0\n",
    "        recalls.append(recall)\n",
    "    return sum(recalls) / len(recalls) * 100.0\n",
    "\n",
    "def macro_precision(actual, predicted):\n",
    "    unique_classes = set(actual)\n",
    "    precisions = []\n",
    "    for class_label in unique_classes:\n",
    "        TP, FP, _ = calculate_metrics_per_class(actual, predicted, class_label)\n",
    "        precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "        precisions.append(precision)\n",
    "    return sum(precisions) / len(precisions) * 100.0\n",
    "\n",
    "def macro_f1_score(actual, predicted):\n",
    "    precision = macro_precision(actual, predicted) / 100.0\n",
    "    recall = macro_recall(actual, predicted) / 100.0\n",
    "    return 2 * (precision * recall) / (precision + recall) * 100.0 if (precision + recall) else 0\n",
    "\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, dataset_validation, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores_cv = list()\n",
    "\tscores_val = list()\n",
    "\t\n",
    "\tscores_cv_recall = list()\n",
    "\tscores_cv_precision = list()\n",
    "\tscores_cv_f1 = list()\n",
    "\n",
    "\tscores_val_recall = list()\n",
    "\tscores_val_precision = list()\n",
    "\tscores_val_f1 = list()\n",
    "\n",
    "\n",
    "\tactual_validation = [row[-1] for row in dataset_validation]\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted_test, predicted_validation = algorithm(train_set, test_set, dataset_validation, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "  \n",
    "\t\taccuracy_test = accuracy_metric(actual, predicted_test)\n",
    "\t\tscores_cv.append(accuracy_test)\n",
    "  \n",
    "\t\tscores_cv_recall.append(macro_recall(actual, predicted_test))\n",
    "\t\tscores_cv_precision.append(macro_precision(actual, predicted_test))\n",
    "\t\tscores_cv_f1.append(macro_f1_score(actual, predicted_test))\n",
    "  \n",
    "\t\taccuracy_val = accuracy_metric(actual_validation, predicted_validation)\n",
    "\t\tscores_val.append(accuracy_val)\n",
    "\t\t\n",
    "\t\tscores_val_recall.append(macro_recall(actual, predicted_validation))\n",
    "\t\tscores_val_precision.append(macro_precision(actual, predicted_validation))\n",
    "\t\tscores_val_f1.append(macro_f1_score(actual, predicted_validation))\n",
    "\n",
    "\treturn scores_cv, scores_val, scores_cv_recall, scores_val_recall, scores_cv_precision, scores_val_precision, scores_cv_f1, scores_val_f1 \n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "\tdistance = 0.0\n",
    "\tfor i in range(len(row1)-1):\n",
    "\t\tdistance += (row1[i] - row2[i])**2\n",
    "\treturn sqrt(distance)\n",
    "\n",
    "# Locate the best matching unit\n",
    "def get_best_matching_unit(codebooks, test_row):\n",
    "\tdistances = list()\n",
    "\tfor codebook in codebooks:\n",
    "\t\tdist = euclidean_distance(codebook, test_row)\n",
    "\t\tdistances.append((codebook, dist))\n",
    "\tdistances.sort(key=lambda tup: tup[1])\n",
    "\treturn distances[0][0]\n",
    "\n",
    "# Make a prediction with codebook vectors\n",
    "def predict(codebooks, test_row):\n",
    "\tbmu = get_best_matching_unit(codebooks, test_row)\n",
    "\treturn bmu[-1]\n",
    "\n",
    "# Create a random codebook vector\n",
    "def random_codebook(train):\n",
    "\tn_records = len(train)\n",
    "\tn_features = len(train[0])\n",
    "\tcodebook = [train[randrange(n_records)][i] for i in range(n_features)]\n",
    "\treturn codebook\n",
    "\n",
    "# Train a set of codebook vectors\n",
    "def train_codebooks(train, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = [random_codebook(train) for i in range(n_codebooks)]\n",
    "\tfor epoch in range(epochs):\n",
    "\t\trate = lrate * (1.0-(epoch/float(epochs)))\n",
    "\t\tfor row in train:\n",
    "\t\t\tbmu = get_best_matching_unit(codebooks, row)\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\terror = row[i] - bmu[i]\n",
    "\t\t\t\tif bmu[-1] == row[-1]:\n",
    "\t\t\t\t\tbmu[i] += rate * error\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbmu[i] -= rate * error\n",
    "\treturn codebooks\n",
    "\n",
    "# LVQ Algorithm\n",
    "def learning_vector_quantization(train, test, validation, n_codebooks, lrate, epochs):\n",
    "\tcodebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n",
    "\tpredictions_test = list()\n",
    "\tpredictions_validation = list()\n",
    "\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions_test.append(output)\n",
    "\tfor row in validation:\n",
    "\t\toutput = predict(codebooks, row)\n",
    "\t\tpredictions_validation.append(output)\n",
    "  \n",
    "\treturn predictions_test, predictions_validation\n",
    "\n",
    "# Adjusting evaluation algorithm with cross validation split to consider F1-score\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in range(len(folds)):\n",
    "\t\ttrain_set = [folds[index] for index in range(len(folds)) if (not (index != fold))]\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in folds[fold]:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in folds[fold]]\n",
    "\t\tf1score = f1_score(actual, predicted, average='weighted')\n",
    "\t\tscores.append(f1score)\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cara_train = df_train[numeric_columns].values  #caracteristicas\n",
    "df_clas_train = df_train['track_genre'].values #classe\n",
    "\n",
    "df_cara_validation = df_validation[numeric_columns].values  #caracteristicas\n",
    "df_clas_validation = df_validation['track_genre'].values #classe\n",
    "\n",
    "df_cara_test = df_test[numeric_columns].values  #caracteristicas\n",
    "df_clas_test = df_test['track_genre'].values #classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 40\n",
      "0.01 50\n",
      "0.01 60\n",
      "{(0.01, 40): {'cv_accuracy': 0.7865497076023391, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.006899558838617011, 'validation_precision': 0.006899558838617011, 'cv_f1': 0.013687683109619469, 'validation_f1': 0.013687683109619469}, (0.01, 50): {'cv_accuracy': 0.8742690058479532, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.00766902636708731, 'validation_precision': 0.00766902636708731, 'cv_f1': 0.015203534847939409, 'validation_f1': 0.015203534847939409}, (0.01, 60): {'cv_accuracy': 0.9239766081871345, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.008105057966553812, 'validation_precision': 0.008105057966553812, 'cv_f1': 0.016060478273497235, 'validation_f1': 0.016060478273497235}}\n",
      "0.2 40\n",
      "0.2 50\n",
      "0.2 60\n",
      "{(0.01, 40): {'cv_accuracy': 0.7865497076023391, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.006899558838617011, 'validation_precision': 0.006899558838617011, 'cv_f1': 0.013687683109619469, 'validation_f1': 0.013687683109619469}, (0.01, 50): {'cv_accuracy': 0.8742690058479532, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.00766902636708731, 'validation_precision': 0.00766902636708731, 'cv_f1': 0.015203534847939409, 'validation_f1': 0.015203534847939409}, (0.01, 60): {'cv_accuracy': 0.9239766081871345, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.008105057966553812, 'validation_precision': 0.008105057966553812, 'cv_f1': 0.016060478273497235, 'validation_f1': 0.016060478273497235}, (0.2, 40): {'cv_accuracy': 0.8625730994152047, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.0075664306966246025, 'validation_precision': 0.0075664306966246025, 'cv_f1': 0.015002379455364484, 'validation_f1': 0.015002379455364484}, (0.2, 50): {'cv_accuracy': 0.8011695906432749, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007027803426695393, 'validation_precision': 0.007027803426695393, 'cv_f1': 0.013943798901517012, 'validation_f1': 0.013943798901517012}, (0.2, 60): {'cv_accuracy': 0.8567251461988304, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007515132861393249, 'validation_precision': 0.007515132861393249, 'cv_f1': 0.014901880892905955, 'validation_f1': 0.014901880892905955}}\n",
      "0.5 40\n",
      "0.5 50\n",
      "0.5 60\n",
      "{(0.01, 40): {'cv_accuracy': 0.7865497076023391, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.006899558838617011, 'validation_precision': 0.006899558838617011, 'cv_f1': 0.013687683109619469, 'validation_f1': 0.013687683109619469}, (0.01, 50): {'cv_accuracy': 0.8742690058479532, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.00766902636708731, 'validation_precision': 0.00766902636708731, 'cv_f1': 0.015203534847939409, 'validation_f1': 0.015203534847939409}, (0.01, 60): {'cv_accuracy': 0.9239766081871345, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.008105057966553812, 'validation_precision': 0.008105057966553812, 'cv_f1': 0.016060478273497235, 'validation_f1': 0.016060478273497235}, (0.2, 40): {'cv_accuracy': 0.8625730994152047, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.0075664306966246025, 'validation_precision': 0.0075664306966246025, 'cv_f1': 0.015002379455364484, 'validation_f1': 0.015002379455364484}, (0.2, 50): {'cv_accuracy': 0.8011695906432749, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007027803426695393, 'validation_precision': 0.007027803426695393, 'cv_f1': 0.013943798901517012, 'validation_f1': 0.013943798901517012}, (0.2, 60): {'cv_accuracy': 0.8567251461988304, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007515132861393249, 'validation_precision': 0.007515132861393249, 'cv_f1': 0.014901880892905955, 'validation_f1': 0.014901880892905955}, (0.5, 40): {'cv_accuracy': 0.8450292397660819, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007412537190930543, 'validation_precision': 0.007412537190930543, 'cv_f1': 0.014700154266114632, 'validation_f1': 0.014700154266114632}, (0.5, 50): {'cv_accuracy': 0.9005847953216375, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007899866625628398, 'validation_precision': 0.007899866625628398, 'cv_f1': 0.01565674752929925, 'validation_f1': 0.01565674752929925}, (0.5, 60): {'cv_accuracy': 0.8567251461988304, 'validation_accuracy': 0.8771929824561402, 'cv_recall': 0.8771929824561402, 'validation_recall': 0.8771929824561402, 'cv_precision': 0.007515132861393247, 'validation_precision': 0.007515132861393247, 'cv_f1': 0.014900682322525893, 'validation_f1': 0.014900682322525893}}\n"
     ]
    }
   ],
   "source": [
    "# Test LVQ on Spotify Dataset\n",
    "n_folds = 5\n",
    "learn_rate = 0.1\n",
    "n_epochs = 20\n",
    "n_codebooks = 30\n",
    "# load and prepare data\n",
    "scores_cv, scores_vali, scores_cv_recall, scores_val_recall, scores_cv_precision, scores_val_precision, scores_cv_f1, scores_val_f1= evaluate_algorithm(df_train, \n",
    "                                                                                                                                                        learning_vector_quantization, n_folds, n_codebooks, learn_rate, n_epochs)\n",
    "print('Scores para cada fold: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "evaluation = {\n",
    "    'cv_accuracy': sum(scores_cv)/float(len(scores_cv)),\n",
    "    'validation_accuracy': sum(scores_vali)/float(len(scores_vali)),\n",
    "    'cv_recall': sum(scores_cv_recall)/float(len(scores_cv_recall)),\n",
    "    'validation_recall': sum(scores_val_recall)/float(len(scores_val_recall)),\n",
    "    'cv_precision': sum(scores_cv_precision)/float(len(scores_cv_precision)),\n",
    "    'validation_precision': sum(scores_val_precision)/float(len(scores_val_precision)),\n",
    "    'cv_f1': sum(scores_cv_f1)/float(len(scores_cv_f1)),\n",
    "    'validation_f1': sum(scores_val_f1)/float(len(scores_val_f1))\n",
    "}\n",
    "\n",
    "\n",
    "print(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores CV: [0.804093567251462, 0.97953216374269, 1.0087719298245614, 0.8625730994152047, 0.8333333333333334]\n",
      "Scores Validation: [0.8771929824561403, 0.8771929824561403, 0.8771929824561403, 0.8771929824561403, 0.8771929824561403]\n",
      "Scores CV Recall: [0.8771929824561403, 0.8771929824561403, 0.8771929824561403, 0.8771929824561403, 0.8771929824561403]\n",
      "Scores Validation Recall: [0.8771929824561403, 0.8771929824561403, 0.8771929824561403, 0.8771929824561403, 0.8771929824561403]\n",
      "Scores CV Precision: [0.00705345234431107, 0.008592387401251667, 0.008848876577408434, 0.0075664306966246025, 0.007309941520467837]\n",
      "Scores Validation Precision: [0.00705345234431107, 0.008592387401251667, 0.008848876577408434, 0.0075664306966246025, 0.007309941520467837]\n",
      "Scores CV F1: [0.013994376804956552, 0.01701807726207077, 0.017521006162823476, 0.01500344570659872, 0.01449905756125852]\n",
      "Scores Validation F1: [0.013994376804956552, 0.01701807726207077, 0.017521006162823476, 0.01500344570659872, 0.01449905756125852]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = df_train.values.tolist()\n",
    "dataset_test = df_test.values.tolist()\n",
    "\n",
    "n_folds = 5\n",
    "learn_rate = 0.1\n",
    "n_epochs = 20\n",
    "n_codebooks = 30\n",
    "\n",
    "scores = evaluate_algorithm(dataset_train, learning_vector_quantization, n_folds, n_codebooks, learn_rate, n_epochs)\n",
    "print('Scores para cada fold: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "print(\"Scores CV:\", scores_cv)\n",
    "print(\"Scores Validation:\", scores_vali)\n",
    "print(\"Scores CV Recall:\", scores_cv_recall)\n",
    "print(\"Scores Validation Recall:\", scores_val_recall)\n",
    "print(\"Scores CV Precision:\", scores_cv_precision)\n",
    "print(\"Scores Validation Precision:\", scores_val_precision)\n",
    "print(\"Scores CV F1:\", scores_cv_f1)\n",
    "print(\"Scores Validation F1:\", scores_val_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_3_'></a>[Decision Tree](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_4_'></a>[MLP](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_5_'></a>[SVM](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Div. de dados atributos e classe\n",
    "df_cara_train = df_train[numeric_columns].values  #caracteristicas\n",
    "df_clas_train = df_train['track_genre'].values #classe\n",
    "\n",
    "df_cara_validation = df_validation[numeric_columns].values  #caracteristicas\n",
    "df_clas_validation = df_validation['track_genre'].values #classe\n",
    "\n",
    "df_cara_test = df_test[numeric_columns].values  #caracteristicas\n",
    "df_clas_test = df_test['track_genre'].values #classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_svm = SVC().fit(df_cara_train,df_clas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_kernels=['poly']\n",
    "lista_c =[2,3]\n",
    "lista_gamma = [2,3,4]\n",
    "\n",
    "# Criando um dicionário com os hiperparâmetros e valores a serem testados\n",
    "param_grid = {'kernel': lista_kernels,'C': lista_c, 'gamma':lista_gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(class_svm, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(df_cara_train,df_clas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23959782, 0.24529967, 0.24655703, 0.24743428, 0.24910085,\n",
       "       0.2461184 , 0.17243205, 0.24614763, 0.2496857 , 0.25003663,\n",
       "       0.25120619, 0.24933483, 0.24588455, 0.17181803, 0.24822371,\n",
       "       0.25141093, 0.25392558, 0.25322383, 0.24965652, 0.24518281,\n",
       "       0.1715256 , 0.25117696, 0.25418874, 0.25462738, 0.25339931,\n",
       "       0.25018284, 0.24287283, 0.17140864, 0.25448113, 0.2567034 ,\n",
       "       0.25576777, 0.25237586, 0.24731738, 0.23977326, 0.17120395,\n",
       "       0.2579023 , 0.25857481, 0.25576782, 0.25217119, 0.24605999,\n",
       "       0.23573809, 0.17088231, 0.26281478, 0.25585556, 0.24801923,\n",
       "       0.24097225, 0.23056255, 0.22348627, 0.17041447])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor parametro: {'C': 100, 'gamma': 2}\n",
      "Melhor resultado: 0.26281478175137607\n"
     ]
    }
   ],
   "source": [
    "print(f'Melhor parametro: {grid.best_params_}')\n",
    "print(f'Melhor resultado: {grid.best_score_}')\n",
    "#lista_kernels=['linear','rbf']\n",
    "#lista_c =[2,3,4,5,7,10,100]\n",
    "#lista_gamma = [2,3,4,5,7,10,100]\n",
    "#Melhor parametro: {'C': 100, 'gamma': 2, 'kernel': 'rbf'}\n",
    "#Melhor resultado: 0.26281478175137607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de Teste\n",
      "{'accuracy': 0.26570175438596494, 'precision': 0.2596195645386902, 'recall': 0.26570175438596494, 'f1': 0.2578082413357941}\n"
     ]
    }
   ],
   "source": [
    "# Predizendo os rótulos dos dados de teste\n",
    "df_clas_pred = grid.best_estimator_.predict(df_cara_test)\n",
    "\n",
    "evaluation={\n",
    "'accuracy': accuracy_score(df_clas_test, df_clas_pred),\n",
    "'precision': precision_score(df_clas_test, df_clas_pred, average='weighted'),\n",
    "'recall': recall_score(df_clas_test, df_clas_pred, average='weighted'),\n",
    "'f1': f1_score(df_clas_test, df_clas_pred, average='weighted')\n",
    "}\n",
    "\n",
    "print(f'Dados de Teste')\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_6_'></a>[Stacking](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_7_'></a>[Random Forest](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
