{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Imports](#toc1_)    \n",
    "- [Read Data](#toc2_)    \n",
    "- [Data Preprocessing](#toc3_)\n",
    "  - [Drop rows with missing values](#toc3_2_)    \n",
    "  - [Removing Categorical Columns](#toc4_)    \n",
    "  - [Split Train and Test Data](#toc5_)    \n",
    "  - [Data Cleaning](#toc6_)    \n",
    "    - [Impute missing numeric data](#toc6_1_)    \n",
    "  - [Data Normalization](#toc7_)    \n",
    "- [Model training](#toc8_)    \n",
    "  - [KNN](#toc8_1_)  \n",
    "  - [LVQ](#toc8_2_)\n",
    "  - [Decision Tree](#toc8_3_)  \n",
    "  - [MLP](#toc8_4_)\n",
    "  - [SVM](#toc8_5_)  \n",
    "  - [Stacking](#toc8_6_)  \n",
    "  - [Random Forest](#toc8_7_)  \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Imports](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/rickvanveen/sklvq.git\n",
      "  Cloning https://github.com/rickvanveen/sklvq.git to c:\\users\\pichau\\appdata\\local\\temp\\pip-req-build-8fam6gx1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/rickvanveen/sklvq.git 'C:\\Users\\pichau\\AppData\\Local\\Temp\\pip-req-build-8fam6gx1'\n",
      "  fatal: unable to access 'https://github.com/rickvanveen/sklvq.git/': Could not resolve host: github.com\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × git clone --filter=blob:none --quiet https://github.com/rickvanveen/sklvq.git 'C:\\Users\\pichau\\AppData\\Local\\Temp\\pip-req-build-8fam6gx1' did not run successfully.\n",
      "  │ exit code: 128\n",
      "  ╰─> See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× git clone --filter=blob:none --quiet https://github.com/rickvanveen/sklvq.git 'C:\\Users\\pichau\\AppData\\Local\\Temp\\pip-req-build-8fam6gx1' did not run successfully.\n",
      "│ exit code: 128\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install -U git+https://github.com/rickvanveen/sklvq.git\n",
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_validate,train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklvq import GLVQ\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, make_scorer\n",
    "import random\n",
    "from random import seed,randrange\n",
    "import requests\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Read Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(heart_disease.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(heart_disease.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Visualize Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Data Preprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Drop rows with missing values](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True, axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Removing Categorical Columns](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 113999 entries, 0 to 113999\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   popularity        113999 non-null  int64  \n",
      " 1   duration_ms       113999 non-null  int64  \n",
      " 2   explicit          113999 non-null  bool   \n",
      " 3   danceability      113999 non-null  float64\n",
      " 4   energy            113999 non-null  float64\n",
      " 5   key               113999 non-null  int64  \n",
      " 6   loudness          113999 non-null  float64\n",
      " 7   mode              113999 non-null  int64  \n",
      " 8   speechiness       113999 non-null  float64\n",
      " 9   acousticness      113999 non-null  float64\n",
      " 10  instrumentalness  113999 non-null  float64\n",
      " 11  liveness          113999 non-null  float64\n",
      " 12  valence           113999 non-null  float64\n",
      " 13  tempo             113999 non-null  float64\n",
      " 14  time_signature    113999 non-null  int64  \n",
      " 15  track_genre       113999 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(5), object(1)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = [\"Unnamed: 0\", \"track_id\", \"track_name\", \"album_name\", \"artists\"]\n",
    "df = df.drop(categorical_columns, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Split Train and Test Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_test_split(df, target_column, validation_size=0.1, test_size=0.1, random_state=42):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_column])\n",
    "    \n",
    "    df_train, df_validation = train_test_split(df_train,\n",
    "                                               test_size=validation_size/(1 - test_size),\n",
    "                                               random_state=random_state,\n",
    "                                               stratify=df_train[target_column])\n",
    "    return df_train, df_validation, df_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 113999 entries, 0 to 113999\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   popularity        113999 non-null  int64  \n",
      " 1   duration_ms       113999 non-null  int64  \n",
      " 2   explicit          113999 non-null  bool   \n",
      " 3   danceability      113999 non-null  float64\n",
      " 4   energy            113999 non-null  float64\n",
      " 5   key               113999 non-null  int64  \n",
      " 6   loudness          113999 non-null  float64\n",
      " 7   mode              113999 non-null  int64  \n",
      " 8   speechiness       113999 non-null  float64\n",
      " 9   acousticness      113999 non-null  float64\n",
      " 10  instrumentalness  113999 non-null  float64\n",
      " 11  liveness          113999 non-null  float64\n",
      " 12  valence           113999 non-null  float64\n",
      " 13  tempo             113999 non-null  float64\n",
      " 14  time_signature    113999 non-null  int64  \n",
      " 15  track_genre       113999 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(5), object(1)\n",
      "memory usage: 14.0+ MB\n",
      "\n",
      " 0.5999964911972913 0.2000017544013544 0.2000017544013544\n"
     ]
    }
   ],
   "source": [
    "df_train, df_validation, df_test = train_validation_test_split(df, \"track_genre\",0.2, 0.2)\n",
    "df.info()\n",
    "\n",
    "print('\\n',len(df_train.values)/float(len(df)),len(df_test.values)/float(len(df)),len(df_validation.values)/float(len(df))) #Garantindo que o percentual ocorre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Data Cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc6_1_'></a>[Impute missing numeric data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "numeric_imputer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = numeric_imputer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = numeric_imputer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = numeric_imputer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_'></a>[Data Normalization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "\n",
    "normalizer.fit(df_train[numeric_columns])\n",
    "\n",
    "df_train[numeric_columns] = normalizer.transform(df_train[numeric_columns])\n",
    "df_validation[numeric_columns] = normalizer.transform(df_validation[numeric_columns])\n",
    "df_test[numeric_columns] = normalizer.transform(df_test[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Div. de dados atributos e classe\n",
    "df_cara_train = df_train[numeric_columns].values  #caracteristicas\n",
    "df_clas_train = df_train['track_genre'].values #classe\n",
    "\n",
    "df_cara_validation = df_validation[numeric_columns].values  #caracteristicas\n",
    "df_clas_validation = df_validation['track_genre'].values #classe\n",
    "\n",
    "df_cara_test = df_test[numeric_columns].values  #caracteristicas\n",
    "df_clas_test = df_test['track_genre'].values #classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Model training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_1_'></a>[KNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cara_train_scaled = df_cara_train\n",
    "df_cara_valid_scaled = df_cara_validation\n",
    "df_cara_test_scaled = df_cara_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1748636  0.15861048 0.16880577 0.17154974 0.17500064 0.17567899\n",
      " 0.17537008 0.17594873 0.17530662 0.17495474 0.17563676 0.17429404\n",
      " 0.17423267 0.17333656 0.17353709 0.1732115  0.17287979 0.17233861\n",
      " 0.17125046 0.17021634 0.1702484  0.16896469 0.16769784 0.16724919\n",
      " 0.16641829 0.16637438 0.16600326 0.16487446 0.16456805 0.16393318\n",
      " 0.16359721 0.16302103 0.16188695 0.16125885 0.16108099 0.16073382\n",
      " 0.16078705 0.15982873 0.1588715  0.15828664 0.18945929 0.17724955\n",
      " 0.18825781 0.19278721 0.19557491 0.19736664 0.19715771 0.19794149\n",
      " 0.19734906 0.1991353  0.19896836 0.19766001 0.19836991 0.19772185\n",
      " 0.19737806 0.19758553 0.19734314 0.19711047 0.19644256 0.19637877\n",
      " 0.19560007 0.19604323 0.19556347 0.19502116 0.1942498  0.19444474\n",
      " 0.19418936 0.19393728 0.19386993 0.19266551 0.19213404 0.19144797\n",
      " 0.19135651 0.19105361 0.19006619 0.18963809 0.19024574 0.19016178\n",
      " 0.18950826 0.1891294 ] \n",
      "\n",
      "\n",
      "Melhor parametro: {'metric': 'manhattan', 'n_neighbors': 19}\n",
      "Melhor resultado: 0.1991353044549033 \n",
      "\n",
      "\n",
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier().fit(df_cara_train,df_clas_train)\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1,81,2),\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(df_cara_train,df_clas_train)\n",
    "\n",
    "with open('KNN_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search,f)\n",
    "\n",
    "print(grid_search.cv_results_['mean_test_score'],'\\n\\n')\n",
    "print(f'Melhor parametro: {grid_search.best_params_}')\n",
    "print(f'Melhor resultado: {grid_search.best_score_}','\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_train.iloc[:, :-1], df_train.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_validation.iloc[:, :-1], df_validation.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_test.iloc[:, :-1], df_test.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])[:, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_test = df_test[\"track_genre\"]\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_test.drop(columns=[\"track_genre\"]))\n",
    "df_clas_pred_proba = grid_search.best_estimator_.predict_proba(df_test.drop(columns=[\"track_genre\"]))\n",
    "\n",
    "evaluation = {\n",
    "    \"accuracy\": accuracy_score(df_clas_test, df_clas_pred),\n",
    "    \"precision\": precision_score(\n",
    "        df_clas_test, df_clas_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"recall\": recall_score(df_clas_test, df_clas_pred, average=\"weighted\"),\n",
    "    \"f1\": f1_score(df_clas_test, df_clas_pred, average=\"weighted\")\n",
    "}\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_2_'></a>[LVQ](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cara_train_scaled = df_cara_train\n",
    "df_cara_valid_scaled = df_cara_validation\n",
    "df_cara_test_scaled = df_cara_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o classificador LVQ\n",
    "lvq = GLVQ()\n",
    "\n",
    "# Criando o dicionário de parâmetros para o grid search\n",
    "param_grid = {\n",
    "    \"prototype_n_per_class\": [1,3],  # Número de protótipos por classe\n",
    "    \"distance_type\": [\"euclidean\"],\n",
    "    \"solver_params\": [{\"max_runs\": 5, \"step_size\": step} for step in [0.1, 0.5]]  # Lista de dicionários para diferentes step_sizes\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "# Criando os scorers personalizados\n",
    "scorers = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision_macro\": make_scorer(precision_score, average='macro', zero_division = 0),\n",
    "    \"recall_macro\": make_scorer(recall_score, average='macro'),\n",
    "    \"f1_macro\": make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(lvq, param_grid, cv=5, scoring=scorers, refit=\"accuracy\")\n",
    "\n",
    "# Treinando o GridSearchCV com os dados de treino escalados\n",
    "grid_search.fit(df_cara_train_scaled, df_clas_train)\n",
    "\n",
    "with open('LVQ_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search,f)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_train.iloc[:, :-1], df_train.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_validation.iloc[:, :-1], df_validation.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros:  {'distance_type': 'euclidean', 'prototype_n_per_class': 3, 'solver_params': {'max_runs': 5, 'step_size': 0.1}}\n",
      "accuracy per fold:  [0.12982456 0.12938596 0.13377193 0.13333333 0.13881579]\n",
      "recall_macro per fold:  [0.12982456 0.12938596 0.13377193 0.13333333 0.13881579]\n",
      "f1_macro per fold:  [0.1140344  0.11959892 0.11408463 0.11836989 0.12084056]\n",
      "precision_macro per fold:  [0.12466036 0.13964932 0.14156033 0.1312936  0.13110597]\n",
      "13.302631578947368\n",
      "13.302631578947368\n",
      "11.738567900442213\n",
      "13.365391757392203\n",
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.score(df_test.iloc[:, :-1], df_test.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_test = df_test[\"track_genre\"]\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_test.drop(columns=[\"track_genre\"]))\n",
    "df_clas_pred_proba = grid_search.best_estimator_.predict_proba(df_test.drop(columns=[\"track_genre\"]))\n",
    "\n",
    "evaluation = {\n",
    "    \"accuracy\": accuracy_score(df_clas_test, df_clas_pred),\n",
    "    \"precision\": precision_score(\n",
    "        df_clas_test, df_clas_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"recall\": recall_score(df_clas_test, df_clas_pred, average=\"weighted\"),\n",
    "    \"f1\": f1_score(df_clas_test, df_clas_pred, average=\"weighted\")\n",
    "}\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_3_'></a>[Decision Tree](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "decision_tree = DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "parameters = {\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "    \"max_depth\": np.arange(5, 10000, 50),\n",
    "    \"min_samples_leaf\": np.arange(1, 10000, 50),\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"max_leaf_nodes\": np.arange(2, 10000, 50),\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    cv=5,\n",
    "    random_state=random_state,\n",
    "    n_iter=500,\n",
    "    n_jobs=-1,\n",
    "    estimator=decision_tree,\n",
    "    param_distributions=parameters,\n",
    ")\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(df_train.iloc[:, :-1], df_train.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_train.iloc[:, :-1], df_train.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_validation.iloc[:, :-1], df_validation.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_test.iloc[:, :-1], df_test.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "\n",
    "def display_metrics(model, x_test, y_test):\n",
    "    predicted = model.predict(x_test)\n",
    "    report = classification_report(y_test, predicted)\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "    print(report)\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, predicted)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        center=True,\n",
    "        annot=True,\n",
    "        yticklabels=model.classes_,\n",
    "        xticklabels=model.classes_,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(\n",
    "    grid_search.best_estimator_, df_validation.iloc[:, :-1], df_validation.iloc[:, -1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_test = df_test[\"track_genre\"]\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_test.drop(columns=[\"track_genre\"]))\n",
    "df_clas_pred_proba = grid_search.best_estimator_.predict_proba(df_test.drop(columns=[\"track_genre\"]))\n",
    "\n",
    "evaluation = {\n",
    "    \"accuracy\": accuracy_score(df_clas_test, df_clas_pred),\n",
    "    \"precision\": precision_score(\n",
    "        df_clas_test, df_clas_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"recall\": recall_score(df_clas_test, df_clas_pred, average=\"weighted\"),\n",
    "    \"f1\": f1_score(df_clas_test, df_clas_pred, average=\"weighted\")\n",
    "}\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_4_'></a>[MLP](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cara_train_scaled = df_cara_train\n",
    "df_cara_valid_scaled = df_cara_validation\n",
    "df_cara_test_scaled = df_cara_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(100,)],\n",
    "    \"activation\": [\"relu\"],# \"tanh\"],\n",
    "    \"solver\": [\"adam\"], #, 'sgd'],\n",
    "    \"alpha\": [0.0001],# 0.01],\n",
    "    \"learning_rate\": [\"adaptive\"],#\"constant\"]\n",
    "    \"max_iter\": [700]#300,500,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', verbose=3)\n",
    "grid_search.fit(df_cara_train, df_clas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3010278  0.30273837]\n",
      "Melhor parametro: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'max_iter': 700, 'solver': 'adam'}\n",
      "Melhor resultado: 0.30273837281631594\n"
     ]
    }
   ],
   "source": [
    "with open('MLP_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search, f)\n",
    "print(grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Melhor parametro: {grid_search.best_params_}')\n",
    "print(f'Melhor resultado: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.score(df_train.iloc[:, :-1], df_train.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_validation.iloc[:, :-1], df_validation.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_test.iloc[:, :-1], df_test.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_test = df_test[\"track_genre\"]\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_test.drop(columns=[\"track_genre\"]))\n",
    "df_clas_pred_proba = grid_search.best_estimator_.predict_proba(df_test.drop(columns=[\"track_genre\"]))\n",
    "\n",
    "evaluation = {\n",
    "    \"accuracy\": accuracy_score(df_clas_test, df_clas_pred),\n",
    "    \"precision\": precision_score(\n",
    "        df_clas_test, df_clas_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"recall\": recall_score(df_clas_test, df_clas_pred, average=\"weighted\"),\n",
    "    \"f1\": f1_score(df_clas_test, df_clas_pred, average=\"weighted\")\n",
    "}\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_5_'></a>[SVM](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_svm = SVC().fit(df_cara_train,df_clas_train)\n",
    "###CUIDADO AO RODAR AS CÉLULAS ABAIXO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_kernels=['rbf']\n",
    "lista_c =[100]\n",
    "lista_gamma = [2]\n",
    "\n",
    "# Criando um dicionário com os hiperparâmetros e valores a serem testados\n",
    "param_grid = {'kernel': lista_kernels,'C': lista_c, 'gamma':lista_gamma}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2449159]\n",
      "Melhor parametro: {'C': 100, 'gamma': 2, 'kernel': 'rbf'}\n",
      "Melhor resultado: 0.24491589853230442\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(class_svm, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(df_cara_train,df_clas_train)\n",
    "\n",
    "with open('SVM_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search,f)\n",
    "print(grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Melhor parametro: {grid_search.best_params_}')\n",
    "print(f'Melhor resultado: {grid_search.best_score_}')\n",
    "#lista_kernels=['linear','rbf']\n",
    "#lista_c =[2,3,4,5,7,10,100]\n",
    "#lista_gamma = [2,3,4,5,7,10,100]\n",
    "#Melhor parametro: {'C': 100, 'gamma': 2, 'kernel': 'rbf'}\n",
    "#Melhor resultado: 0.26281478175137607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de Teste\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n",
      "{'accuracy': make_scorer(accuracy_score), 'recall_macro': make_scorer(recall_score, average=macro), 'f1_macro': make_scorer(f1_score, average=macro), 'precision_macro': make_scorer(precision_score, average=macro, zero_division=0)}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.score(df_train.iloc[:, :-1], df_train.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_validation.iloc[:, :-1], df_validation.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_test.iloc[:, :-1], df_test.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_test = df_test[\"track_genre\"]\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_test.drop(columns=[\"track_genre\"]))\n",
    "df_clas_pred_proba = grid_search.best_estimator_.predict_proba(df_test.drop(columns=[\"track_genre\"]))\n",
    "\n",
    "evaluation = {\n",
    "    \"accuracy\": accuracy_score(df_clas_test, df_clas_pred),\n",
    "    \"precision\": precision_score(\n",
    "        df_clas_test, df_clas_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"recall\": recall_score(df_clas_test, df_clas_pred, average=\"weighted\"),\n",
    "    \"f1\": f1_score(df_clas_test, df_clas_pred, average=\"weighted\")\n",
    "}\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_6_'></a>[Stacking_Pikle Import](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics for each df type\n",
    "def testCurrentConfiguration(stacking_clf, estimator, final_estimator):\n",
    "    print(\"---------------------\")\n",
    "    print(\"running for \", final_estimator)\n",
    "    print([i[0] for i in estimator])\n",
    "    df_clas_pred_test = stacking_clf.predict(df_cara_test)\n",
    "    df_clas_pred_train = stacking_clf.predict(df_cara_train)\n",
    "    df_clas_pred_val = stacking_clf.predict(df_cara_validation)\n",
    "\n",
    "    evaluation_test={\n",
    "    'accuracy': accuracy_score(df_clas_test, df_clas_pred_test),\n",
    "    'precision': precision_score(df_clas_test, df_clas_pred_test, average='weighted'),\n",
    "    'recall': recall_score(df_clas_test, df_clas_pred_test, average='weighted'),\n",
    "    'f1': f1_score(df_clas_test, df_clas_pred_test, average='weighted')\n",
    "    }\n",
    "    evaluation_train={\n",
    "    'accuracy': accuracy_score(df_clas_train, df_clas_pred_train),\n",
    "    'precision': precision_score(df_clas_train, df_clas_pred_train, average='weighted'),\n",
    "    'recall': recall_score(df_clas_train, df_clas_pred_train, average='weighted'),\n",
    "    'f1': f1_score(df_clas_train, df_clas_pred_train, average='weighted')\n",
    "    }\n",
    "    evaluation_val={\n",
    "    'accuracy': accuracy_score(df_clas_validation, df_clas_pred_val),\n",
    "    'precision': precision_score(df_clas_validation, df_clas_pred_val, average='weighted'),\n",
    "    'recall': recall_score(df_clas_validation, df_clas_pred_val, average='weighted'),\n",
    "    'f1': f1_score(df_clas_validation, df_clas_pred_val, average='weighted')\n",
    "    }\n",
    "\n",
    "    print(f'Dados de Teste')\n",
    "    print(evaluation_test)\n",
    "    print(f'Dados de Treino')\n",
    "    print(evaluation_train)\n",
    "    print(f'Dados de Validaçao')\n",
    "    print(evaluation_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained base classifiers\n",
    "lvq = None\n",
    "mlp = None\n",
    "knn = None\n",
    "\n",
    "with (open('.\\\\modelos\\\\LVQ_model_searcher.pkl', 'rb')) as f:\n",
    "    lvq = pickle.load(f)\n",
    "with (open('.\\\\modelos\\\\MLP_model_searcher.pkl', 'rb')) as f:\n",
    "    mlp = pickle.load(f)\n",
    "with (open('.\\\\modelos\\\\KNN_model_searcher.pkl', 'rb')) as f:\n",
    "    knn = pickle.load(f)\n",
    "\n",
    "# Create possible combinations of classifiers\n",
    "estimators_combination = [\n",
    "    [\n",
    "        ('mlp', mlp),\n",
    "        ('knn', knn),\n",
    "        ('lvq', lvq),\n",
    "    ],\n",
    "    [\n",
    "        ('mlp', mlp),\n",
    "        ('knn', knn),\n",
    "    ],\n",
    "    [\n",
    "        ('mlp', mlp),\n",
    "        ('lvq', lvq),\n",
    "    ],\n",
    "    [\n",
    "        ('knn', knn),\n",
    "        ('lvq', lvq),\n",
    "    ],\n",
    "]\n",
    "# List of final classifiers\n",
    "final_estimator_combination = [LogisticRegression(), DecisionTreeClassifier()]\n",
    "\n",
    "# Test each pair of <estimators, final_estimator> to check their metrics\n",
    "for estimator in estimators_combination:\n",
    "    for final_estimator in final_estimator_combination:\n",
    "        # Setting prefit since we'll use the pretrained models above\n",
    "        stacking_clf = StackingClassifier(estimators=estimator, cv=\"prefit\", final_estimator = final_estimator)\n",
    "        # Train the stacking classifier\n",
    "        stacking_clf.fit(df_cara_train, df_clas_train)\n",
    "        testCurrentConfiguration(stacking_clf, estimator, final_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "LogisticRegression() and ['mlp', 'knn', 'lvq']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.33973684210526317, 'precision': 0.34139945210280004, 'recall': 0.33973684210526317, 'f1': 0.3336826841814082}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.349093567251462, 'precision': 0.35105385289525554, 'recall': 0.349093567251462, 'f1': 0.34309435758690215}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.3333333333333333, 'precision': 0.33351549805734143, 'recall': 0.3333333333333333, 'f1': 0.32617102272847603}\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "DecisionTreeClassifier() and ['mlp', 'knn', 'lvq']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.2162280701754386, 'precision': 0.22048676762124503, 'recall': 0.2162280701754386, 'f1': 0.21740034305197142}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.9150877192982456, 'precision': 0.9219857008466373, 'recall': 0.9150877192982456, 'f1': 0.9146622179174141}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.21070175438596492, 'precision': 0.21447817161106542, 'recall': 0.21070175438596492, 'f1': 0.21172910144200224}\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "LogisticRegression() and ['mlp', 'knn']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.33964912280701753, 'precision': 0.34128038961699103, 'recall': 0.33964912280701753, 'f1': 0.3335919922101997}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.349093567251462, 'precision': 0.35107340335988624, 'recall': 0.349093567251462, 'f1': 0.34309757660798107}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.3335087719298246, 'precision': 0.33372436732779615, 'recall': 0.3335087719298246, 'f1': 0.32635466564214843}\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "DecisionTreeClassifier() and ['mlp', 'knn']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.218859649122807, 'precision': 0.2256198364329918, 'recall': 0.218859649122807, 'f1': 0.2211579625357569}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.9150877192982456, 'precision': 0.9219857008466373, 'recall': 0.9150877192982456, 'f1': 0.9146622179174141}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.2119298245614035, 'precision': 0.21741307507188387, 'recall': 0.2119298245614035, 'f1': 0.2137076792286073}\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "LogisticRegression() and ['mlp', 'lvq']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.3175438596491228, 'precision': 0.31734137211142344, 'recall': 0.3175438596491228, 'f1': 0.3064514645508661}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.3166374269005848, 'precision': 0.31398891231022374, 'recall': 0.3166374269005848, 'f1': 0.3062036094875135}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.30929824561403507, 'precision': 0.3044883153326813, 'recall': 0.30929824561403507, 'f1': 0.2987353639361971}\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "DecisionTreeClassifier() and ['mlp', 'lvq']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.21210526315789474, 'precision': 0.21589840017411255, 'recall': 0.21210526315789474, 'f1': 0.21287484022042927}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.9150877192982456, 'precision': 0.9219857008466373, 'recall': 0.9150877192982456, 'f1': 0.9146622179174141}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.20649122807017545, 'precision': 0.21075318095663353, 'recall': 0.20649122807017545, 'f1': 0.20774644656404842}\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "LogisticRegression() and ['knn', 'lvq']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.2893859649122807, 'precision': 0.28885355731186063, 'recall': 0.2893859649122807, 'f1': 0.28051175029905395}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.29403508771929826, 'precision': 0.29360649609651057, 'recall': 0.29403508771929826, 'f1': 0.28516542279260515}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.28192982456140353, 'precision': 0.27751455749852805, 'recall': 0.28192982456140353, 'f1': 0.2710909125079709}\n",
    "\n",
    "---------------------\n",
    "\n",
    "Dados os parametros:\n",
    "\n",
    "DecisionTreeClassifier() and ['knn', 'lvq']\n",
    "\n",
    "Dados de Teste\n",
    "\n",
    "{'accuracy': 0.17587719298245613, 'precision': 0.1814274155830884, 'recall': 0.17587719298245613, 'f1': 0.17769330577152145}\n",
    "Dados de Treino\n",
    "\n",
    "{'accuracy': 0.9150877192982456, 'precision': 0.9219857008466373, 'recall': 0.9150877192982456, 'f1': 0.9146622179174141}\n",
    "Dados de Validaçao\n",
    "\n",
    "{'accuracy': 0.17394736842105263, 'precision': 0.17779027035188788, 'recall': 0.17394736842105263, 'f1': 0.17520835874535154}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Parameters\n",
    "After running the comparison above we noticed that the best configuration for our model was:\n",
    "- final_estimator: LogisticRegression()\n",
    "- estimators: ['mlp', 'knn', 'lvq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_combination = [\n",
    "    ('mlp', mlp),\n",
    "    ('knn', knn),\n",
    "    ('lvq', lvq),\n",
    "]\n",
    "final_estimator = LogisticRegression()\n",
    "stacking_clf = StackingClassifier(estimators=estimator, cv=\"prefit\", final_estimator = final_estimator)\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(df_cara_train, df_clas_train)\n",
    "\n",
    "with open('stacking_model_searcher.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc8_7_'></a>[Random Forest](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "\n",
    "decision_tree =RandomForestClassifier(\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\": np.arange(5, 1000, 10),\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"max_depth\": np.arange(1, 50),\n",
    "    \"min_samples_split\": np.arange(2, 100),\n",
    "    \"min_samples_leaf\": np.arange(1, 100)\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    cv=5,\n",
    "    random_state=random_state,\n",
    "    n_iter=15,\n",
    "    n_jobs=7,\n",
    "    estimator=decision_tree,\n",
    "    param_distributions=parameters,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(df_train.iloc[:, :-1], df_train.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_train.iloc[:, :-1], df_train.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_validation.iloc[:, :-1], df_validation.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.score(df_test.iloc[:, :-1], df_test.iloc[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([grid_search.cv_results_[f\"split{i}_test_score\"] for i in range(grid_search.cv)])[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_test = df_test[\"track_genre\"]\n",
    "df_clas_pred = grid_search.best_estimator_.predict(df_test.drop(columns=[\"track_genre\"]))\n",
    "df_clas_pred_proba = grid_search.best_estimator_.predict_proba(df_test.drop(columns=[\"track_genre\"]))\n",
    "\n",
    "evaluation = {\n",
    "    \"accuracy\": accuracy_score(df_clas_test, df_clas_pred),\n",
    "    \"precision\": precision_score(\n",
    "        df_clas_test, df_clas_pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"recall\": recall_score(df_clas_test, df_clas_pred, average=\"weighted\"),\n",
    "    \"f1\": f1_score(df_clas_test, df_clas_pred, average=\"weighted\")\n",
    "}\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest.pkl', 'rb') as f:\n",
    "    grid_search_load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
